{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### work in this repository was inspired by \n",
    "##### https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe/blob/main/README_EN.md\n",
    "##### https://www.youtube.com/watch?v=a99p_fAr6e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# this is just to make sure that the results are reproducible to anyone that runs the code lol.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data formatting\n",
    "https://devqa.io/python-convert-csv-file-to-list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed\n",
      "['data/train/closed\\\\eye.csv']\n",
      "1\n",
      "[216.85504913330078, 152.74354934692383, 4.718387126922607, 300.8012008666992, 160.9013557434082, -11.97317361831665, 244.46121215820312, 160.7812786102295, -10.024803876876831, 275.7936096191406, 160.86087226867676, -15.670033693313599, 269.5697784423828, 157.55355834960938, -24.473342895507812, 237.7887725830078, 156.99408531188965, -17.94538140296936, 493.7444305419922, 180.18958568572998, 0.06788171827793121, 409.0971374511719, 173.22411060333252, -14.07525897026062, 464.72591400146484, 182.40240097045898, -13.882994651794434, 433.48480224609375, 176.8485403060913, -18.609195947647095, 441.343994140625, 172.87556648254395, -27.493326663970947, 473.0286407470703, 178.6200714111328, -22.001070976257324]\n",
      "training set:\n",
      "566\n",
      "566\n",
      "open\n",
      "['data/train/open\\\\eye.csv']\n",
      "1\n",
      "[447.63343811035156, 28.05812358856201, -4.567684829235077, 484.0408706665039, 36.453402042388916, -7.2414761781692505, 459.1893768310547, 35.93154430389404, -9.6647310256958, 472.6679229736328, 37.034854888916016, -10.442172288894653, 471.0403823852539, 24.943517446517944, -13.987268209457397, 457.2993850708008, 23.9244282245636, -12.965084314346313, 566.2680053710938, 38.49304676055908, 10.285605192184448, 530.4252243041992, 38.840975761413574, -0.4506131261587143, 555.3429412841797, 44.3990421295166, 2.7804437279701233, 541.7350769042969, 42.780518531799316, -1.00928895175457, 546.0835266113281, 31.29408359527588, -4.312244951725006, 559.8727416992188, 33.692049980163574, -0.15087420120835304]\n",
      "training set:\n",
      "1063\n",
      "1063\n",
      "closed\n",
      "['data/test/closed\\\\eye.csv']\n",
      "1\n",
      "[262.9005241394043, 295.2817726135254, 3.9409154653549194, 302.97351837158203, 296.4185428619385, -6.174900531768799, 277.54432678222656, 299.472599029541, -3.3033421635627747, 292.52296447753906, 297.73109436035156, -6.945088505744934, 288.3000946044922, 296.1056327819824, -11.694833040237427, 272.7345275878906, 297.2181987762451, -7.654852271080017, 401.0587692260742, 295.25407791137695, -6.597258448600769, 359.37400817871094, 298.760404586792, -10.992343425750732, 386.1029815673828, 298.8584518432617, -12.129522562026978, 370.7989501953125, 298.6182975769043, -13.651512861251831, 374.4627380371094, 295.8701705932617, -18.604719638824463, 389.9569320678711, 296.2401008605957, -16.788511276245117]\n",
      "training set:\n",
      "413\n",
      "413\n",
      "open\n",
      "['data/test/open\\\\eye.csv']\n",
      "1\n",
      "[68.38025093078613, 253.45404624938965, 36.768176555633545, 104.19914245605469, 254.0632438659668, 17.92460560798645, 77.38699913024902, 258.4009838104248, 24.965622425079346, 90.80703735351562, 258.02109718322754, 18.2034969329834, 89.78808403015137, 246.63897514343262, 16.906174421310425, 75.76205253601074, 248.09995651245117, 23.74375581741333, 201.04564666748047, 243.45179557800293, -2.245694100856781, 157.0121192932129, 250.2655792236328, 0.37336017936468124, 187.92926788330078, 251.59475326538086, -7.734711170196533, 172.06558227539062, 252.7577018737793, -6.43265962600708, 172.76878356933594, 239.90967750549316, -8.34401547908783, 189.3697166442871, 239.52392578125, -9.75212574005127]\n",
      "training set:\n",
      "817\n",
      "817\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "labels = {0: \"closed\", 1: \"open\"}\n",
    "\n",
    "Y_train = [] #1d array, (big_number, 1)\n",
    "X_train = [] #2d array, (big_number, 42)\n",
    "def data_append_train(data, X_train = X_train, Y_train = Y_train):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/train\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_train.append([float(num) for num in row])\n",
    "                Y_train.append(label)\n",
    "    \n",
    "    print(X_train[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_train))\n",
    "    print(len(Y_train))    \n",
    "\n",
    "Y_test = []\n",
    "X_test = []\n",
    "def data_append_test(data, X_test = X_test, Y_test = Y_test):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/test\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_test.append([float(num) for num in row])\n",
    "                Y_test.append(label)\n",
    "    \n",
    "    print(X_test[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_test))\n",
    "    print(len(Y_test))    \n",
    "#i'll only use the train test.\n",
    "data = \"data\"\n",
    "data_append_train(data)\n",
    "data_append_test(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since I already have the training dataset split, I just need to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.10942459e+02,  1.76260071e+02,  2.64112639e+01, ...,\n",
       "          3.26990395e+02,  1.78720236e+02,  5.03394365e+00],\n",
       "        [ 6.31320477e+01,  1.98462238e+02,  4.23670578e+01, ...,\n",
       "          1.31166334e+02,  1.39122362e+02, -2.56325459e+01],\n",
       "        [ 2.94060850e+01,  2.17488499e+02,  2.33728623e+01, ...,\n",
       "          1.44218750e+02,  2.09302411e+02,  5.02811491e+00],\n",
       "        ...,\n",
       "        [ 8.95617676e+01,  2.18508797e+02,  3.17117834e+01, ...,\n",
       "          2.07774029e+02,  1.84710073e+02, -6.60983962e-03],\n",
       "        [ 2.08185616e+02,  1.69075570e+02, -1.36326253e+00, ...,\n",
       "          4.92963905e+02,  2.00928783e+02, -3.51337385e+01],\n",
       "        [ 2.36047411e+01,  2.18941598e+02,  2.21065569e+01, ...,\n",
       "          1.38865423e+02,  2.09952192e+02,  3.81412655e+00]]),\n",
       " array([0, 1, 1, ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_df_train = pd.DataFrame(X_train)\n",
    "# Y_df_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# X_df_test = pd.DataFrame(X_test)\n",
    "# Y_df_test = pd.DataFrame(Y_test)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "# \n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# the random_state is so that both shuffle perform the same.\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_test,Y_test = shuffle(X_test, Y_test, random_state=0)\n",
    "# X_train.shape, Y_train.shape\n",
    "X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "# model_save_path = \"model/\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model/eye_model/model.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    #everytime the accuracy gets better, it saves\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1063, 36)\n",
      "Shape of X_test: (817, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    # I will just change the input \n",
    "    Dense(units=32, activation='relu', input_shape=(36,)),\n",
    "    Dense(units=32, activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.05)),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=16, activation='relu', \n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    # Dropout(0.2),\n",
    "    Dense(units=NUM_CLASSES, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "32/34 [===========================>..] - ETA: 0s - loss: 12.5455 - accuracy: 0.6074INFO:tensorflow:Assets written to: model/eye_model\\model.01-0.43\\assets\n",
      "34/34 [==============================] - 6s 160ms/step - loss: 12.4527 - accuracy: 0.6058 - val_loss: 9.7799 - val_accuracy: 0.4333\n",
      "Epoch 2/150\n",
      "31/34 [==========================>...] - ETA: 0s - loss: 9.2037 - accuracy: 0.6371INFO:tensorflow:Assets written to: model/eye_model\\model.02-0.48\\assets\n",
      "34/34 [==============================] - 4s 134ms/step - loss: 9.1837 - accuracy: 0.6265 - val_loss: 8.4237 - val_accuracy: 0.4798\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 7.7769 - accuracy: 0.6444 - val_loss: 7.7140 - val_accuracy: 0.4798\n",
      "Epoch 4/150\n",
      "32/34 [===========================>..] - ETA: 0s - loss: 6.6479 - accuracy: 0.6475INFO:tensorflow:Assets written to: model/eye_model\\model.04-0.50\\assets\n",
      "34/34 [==============================] - 4s 134ms/step - loss: 6.6228 - accuracy: 0.6538 - val_loss: 6.6913 - val_accuracy: 0.4994\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 5.6195 - accuracy: 0.6914 - val_loss: 5.5683 - val_accuracy: 0.4774\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 4.8158 - accuracy: 0.6736 - val_loss: 4.7862 - val_accuracy: 0.4565\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 4.1518 - accuracy: 0.7093 - val_loss: 4.2208 - val_accuracy: 0.4994\n",
      "Epoch 8/150\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 3.6549 - accuracy: 0.7474INFO:tensorflow:Assets written to: model/eye_model\\model.08-0.57\\assets\n",
      "34/34 [==============================] - 5s 143ms/step - loss: 3.6099 - accuracy: 0.7244 - val_loss: 3.7630 - val_accuracy: 0.5704\n",
      "Epoch 9/150\n",
      "29/34 [========================>.....] - ETA: 0s - loss: 3.2119 - accuracy: 0.7608INFO:tensorflow:Assets written to: model/eye_model\\model.09-0.61\\assets\n",
      "34/34 [==============================] - 5s 164ms/step - loss: 3.2099 - accuracy: 0.7432 - val_loss: 3.4452 - val_accuracy: 0.6132\n",
      "Epoch 10/150\n",
      "27/34 [======================>.......] - ETA: 0s - loss: 3.0063 - accuracy: 0.7454INFO:tensorflow:Assets written to: model/eye_model\\model.10-0.70\\assets\n",
      "34/34 [==============================] - 5s 152ms/step - loss: 2.9770 - accuracy: 0.7535 - val_loss: 3.1105 - val_accuracy: 0.6965\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 2.7961 - accuracy: 0.7554 - val_loss: 3.0904 - val_accuracy: 0.5936\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 2.6327 - accuracy: 0.7780 - val_loss: 2.7987 - val_accuracy: 0.6463\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 2.4804 - accuracy: 0.7930 - val_loss: 2.6690 - val_accuracy: 0.6818\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 2.3616 - accuracy: 0.7808 - val_loss: 2.6800 - val_accuracy: 0.6854\n",
      "Epoch 15/150\n",
      "28/34 [=======================>......] - ETA: 0s - loss: 2.2474 - accuracy: 0.8069INFO:tensorflow:Assets written to: model/eye_model\\model.15-0.75\\assets\n",
      "34/34 [==============================] - 5s 157ms/step - loss: 2.2303 - accuracy: 0.8090 - val_loss: 2.4882 - val_accuracy: 0.7515\n",
      "Epoch 16/150\n",
      "29/34 [========================>.....] - ETA: 0s - loss: 2.1089 - accuracy: 0.8103INFO:tensorflow:Assets written to: model/eye_model\\model.16-0.78\\assets\n",
      "34/34 [==============================] - 5s 145ms/step - loss: 2.0958 - accuracy: 0.8147 - val_loss: 2.4541 - val_accuracy: 0.7809\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 2.0033 - accuracy: 0.8373 - val_loss: 2.2780 - val_accuracy: 0.6707\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 1.9324 - accuracy: 0.8119 - val_loss: 2.2140 - val_accuracy: 0.7466\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 1.7779 - accuracy: 0.8617 - val_loss: 2.0900 - val_accuracy: 0.7515\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 1.6802 - accuracy: 0.8749 - val_loss: 2.2379 - val_accuracy: 0.6769\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 1.6791 - accuracy: 0.8382 - val_loss: 2.0132 - val_accuracy: 0.7319\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 1.5955 - accuracy: 0.8420 - val_loss: 2.0005 - val_accuracy: 0.7013\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 1.5190 - accuracy: 0.8617 - val_loss: 1.8114 - val_accuracy: 0.7528\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4541 - accuracy: 0.8739INFO:tensorflow:Assets written to: model/eye_model\\model.24-0.79\\assets\n",
      "34/34 [==============================] - 5s 144ms/step - loss: 1.4541 - accuracy: 0.8739 - val_loss: 1.6693 - val_accuracy: 0.7907\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 1.4197 - accuracy: 0.8664 - val_loss: 1.8912 - val_accuracy: 0.6879\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 1.3500 - accuracy: 0.8711 - val_loss: 1.6910 - val_accuracy: 0.7515\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 1.3139 - accuracy: 0.8683 - val_loss: 1.7184 - val_accuracy: 0.6756\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 1.2535 - accuracy: 0.8786 - val_loss: 1.6645 - val_accuracy: 0.7821\n",
      "Epoch 29/150\n",
      "32/34 [===========================>..] - ETA: 0s - loss: 1.2146 - accuracy: 0.8975INFO:tensorflow:Assets written to: model/eye_model\\model.29-0.82\\assets\n",
      "34/34 [==============================] - 5s 145ms/step - loss: 1.2123 - accuracy: 0.8975 - val_loss: 1.5704 - val_accuracy: 0.8237\n",
      "Epoch 30/150\n",
      "28/34 [=======================>......] - ETA: 0s - loss: 1.1551 - accuracy: 0.9018INFO:tensorflow:Assets written to: model/eye_model\\model.30-0.83\\assets\n",
      "34/34 [==============================] - 5s 145ms/step - loss: 1.1481 - accuracy: 0.9031 - val_loss: 1.4984 - val_accuracy: 0.8286\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 1.1169 - accuracy: 0.9022 - val_loss: 1.6075 - val_accuracy: 0.7319\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 1.1706 - accuracy: 0.8589 - val_loss: 1.4781 - val_accuracy: 0.7650\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 1.0760 - accuracy: 0.8890 - val_loss: 1.4949 - val_accuracy: 0.8213\n",
      "Epoch 34/150\n",
      "30/34 [=========================>....] - ETA: 0s - loss: 0.9923 - accuracy: 0.9146INFO:tensorflow:Assets written to: model/eye_model\\model.34-0.84\\assets\n",
      "34/34 [==============================] - 5s 154ms/step - loss: 0.9997 - accuracy: 0.9087 - val_loss: 1.3773 - val_accuracy: 0.8446\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.9457 - accuracy: 0.9351 - val_loss: 1.4992 - val_accuracy: 0.8360\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.9873 - accuracy: 0.8928 - val_loss: 1.3794 - val_accuracy: 0.7699\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.9674 - accuracy: 0.9031 - val_loss: 1.2786 - val_accuracy: 0.8274\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.8510 - accuracy: 0.9407 - val_loss: 1.3352 - val_accuracy: 0.8360\n",
      "Epoch 39/150\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 0.9556 - accuracy: 0.8762INFO:tensorflow:Assets written to: model/eye_model\\model.39-0.86\\assets\n",
      "34/34 [==============================] - 5s 149ms/step - loss: 0.9505 - accuracy: 0.8796 - val_loss: 1.2358 - val_accuracy: 0.8556\n",
      "Epoch 40/150\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 0.8386 - accuracy: 0.9350INFO:tensorflow:Assets written to: model/eye_model\\model.40-0.86\\assets\n",
      "34/34 [==============================] - 6s 167ms/step - loss: 0.8275 - accuracy: 0.9407 - val_loss: 1.1485 - val_accuracy: 0.8592\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.8441 - accuracy: 0.9200 - val_loss: 1.5723 - val_accuracy: 0.7980\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.8570 - accuracy: 0.9040 - val_loss: 1.2211 - val_accuracy: 0.8556\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.8570 - accuracy: 0.8928 - val_loss: 0.9419 - val_accuracy: 0.8311\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.8387 - accuracy: 0.8946 - val_loss: 1.1022 - val_accuracy: 0.8029\n",
      "Epoch 45/150\n",
      "28/34 [=======================>......] - ETA: 0s - loss: 0.7708 - accuracy: 0.9275INFO:tensorflow:Assets written to: model/eye_model\\model.45-0.87\\assets\n",
      "34/34 [==============================] - 6s 176ms/step - loss: 0.7587 - accuracy: 0.9341 - val_loss: 1.1452 - val_accuracy: 0.8739\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.6641 - accuracy: 0.9708 - val_loss: 1.1920 - val_accuracy: 0.8641\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.6540 - accuracy: 0.9633 - val_loss: 1.1137 - val_accuracy: 0.8617\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7307 - accuracy: 0.9238 - val_loss: 1.1670 - val_accuracy: 0.7625\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.7192 - accuracy: 0.9182 - val_loss: 1.0704 - val_accuracy: 0.8482\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 0.6238 - accuracy: 0.9539 - val_loss: 1.0685 - val_accuracy: 0.8372\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 1s 21ms/step - loss: 0.6021 - accuracy: 0.9586 - val_loss: 1.1209 - val_accuracy: 0.7980\n",
      "Epoch 52/150\n",
      "30/34 [=========================>....] - ETA: 0s - loss: 0.5841 - accuracy: 0.9615INFO:tensorflow:Assets written to: model/eye_model\\model.52-0.88\\assets\n",
      "34/34 [==============================] - 5s 155ms/step - loss: 0.5822 - accuracy: 0.9614 - val_loss: 1.0920 - val_accuracy: 0.8764\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7282 - accuracy: 0.8956 - val_loss: 1.5816 - val_accuracy: 0.7038\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.7467 - accuracy: 0.8937 - val_loss: 1.1108 - val_accuracy: 0.8225\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.5785 - accuracy: 0.9643 - val_loss: 1.0425 - val_accuracy: 0.8617\n",
      "Epoch 56/150\n",
      "29/34 [========================>.....] - ETA: 0s - loss: 0.6583 - accuracy: 0.9030INFO:tensorflow:Assets written to: model/eye_model\\model.56-0.88\\assets\n",
      "34/34 [==============================] - 5s 138ms/step - loss: 0.6587 - accuracy: 0.9031 - val_loss: 1.0363 - val_accuracy: 0.8776\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.5981 - accuracy: 0.9351 - val_loss: 0.9743 - val_accuracy: 0.8715\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.5760 - accuracy: 0.9445 - val_loss: 1.0130 - val_accuracy: 0.8201\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.5065 - accuracy: 0.9708 - val_loss: 0.9925 - val_accuracy: 0.8739\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.4673 - accuracy: 0.9849 - val_loss: 1.1604 - val_accuracy: 0.8739\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.4863 - accuracy: 0.9661 - val_loss: 1.1005 - val_accuracy: 0.7968\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.5561 - accuracy: 0.9341 - val_loss: 0.9625 - val_accuracy: 0.8752\n",
      "Epoch 63/150\n",
      "32/34 [===========================>..] - ETA: 0s - loss: 0.4796 - accuracy: 0.9707INFO:tensorflow:Assets written to: model/eye_model\\model.63-0.90\\assets\n",
      "34/34 [==============================] - 6s 188ms/step - loss: 0.4775 - accuracy: 0.9718 - val_loss: 0.9235 - val_accuracy: 0.9033\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.4377 - accuracy: 0.9793 - val_loss: 1.0277 - val_accuracy: 0.8531\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.5202 - accuracy: 0.9407 - val_loss: 1.1588 - val_accuracy: 0.8397\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.4812 - accuracy: 0.9548 - val_loss: 1.0060 - val_accuracy: 0.8947\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3971 - accuracy: 0.9897 - val_loss: 1.0807 - val_accuracy: 0.8788\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.4218 - accuracy: 0.9765 - val_loss: 1.0142 - val_accuracy: 0.8923\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.4694 - accuracy: 0.9511 - val_loss: 1.2979 - val_accuracy: 0.8360\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.5665 - accuracy: 0.8956 - val_loss: 1.0570 - val_accuracy: 0.8531\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.5221 - accuracy: 0.9238 - val_loss: 0.8599 - val_accuracy: 0.8800\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 0.3931 - accuracy: 0.9859 - val_loss: 0.8553 - val_accuracy: 0.8898\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3649 - accuracy: 0.9878 - val_loss: 1.0349 - val_accuracy: 0.8800\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.3733 - accuracy: 0.9737 - val_loss: 0.9987 - val_accuracy: 0.8984\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.4068 - accuracy: 0.9586 - val_loss: 0.9243 - val_accuracy: 0.8715\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.4486 - accuracy: 0.9398 - val_loss: 0.7940 - val_accuracy: 0.8837\n",
      "Epoch 77/150\n",
      "31/34 [==========================>...] - ETA: 0s - loss: 0.3684 - accuracy: 0.9728INFO:tensorflow:Assets written to: model/eye_model\\model.77-0.91\\assets\n",
      "34/34 [==============================] - 6s 178ms/step - loss: 0.3676 - accuracy: 0.9727 - val_loss: 0.8766 - val_accuracy: 0.9094\n",
      "Epoch 78/150\n",
      "30/34 [=========================>....] - ETA: 0s - loss: 0.3316 - accuracy: 0.9885INFO:tensorflow:Assets written to: model/eye_model\\model.78-0.92\\assets\n",
      "34/34 [==============================] - 6s 169ms/step - loss: 0.3311 - accuracy: 0.9878 - val_loss: 0.8202 - val_accuracy: 0.9204\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.3348 - accuracy: 0.9831 - val_loss: 0.8731 - val_accuracy: 0.8886\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3164 - accuracy: 0.9887 - val_loss: 1.0820 - val_accuracy: 0.8898\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.5141 - accuracy: 0.9229 - val_loss: 0.7922 - val_accuracy: 0.8715\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3343 - accuracy: 0.9849 - val_loss: 0.9268 - val_accuracy: 0.8274\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.4449 - accuracy: 0.9313 - val_loss: 0.8246 - val_accuracy: 0.8752\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.3269 - accuracy: 0.9765 - val_loss: 0.8713 - val_accuracy: 0.8935\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2995 - accuracy: 0.9859 - val_loss: 1.0462 - val_accuracy: 0.7834\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.3550 - accuracy: 0.9643 - val_loss: 1.0116 - val_accuracy: 0.7882\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.3649 - accuracy: 0.9567 - val_loss: 0.8270 - val_accuracy: 0.8715\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3045 - accuracy: 0.9840 - val_loss: 0.9788 - val_accuracy: 0.8923\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3003 - accuracy: 0.9765 - val_loss: 0.9114 - val_accuracy: 0.9094\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3636 - accuracy: 0.9492 - val_loss: 0.7649 - val_accuracy: 0.9070\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2909 - accuracy: 0.9802 - val_loss: 0.7103 - val_accuracy: 0.8898\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 0.2834 - accuracy: 0.9812 - val_loss: 1.0356 - val_accuracy: 0.9106\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2918 - accuracy: 0.9765 - val_loss: 0.8631 - val_accuracy: 0.9119\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2538 - accuracy: 0.9925 - val_loss: 0.9735 - val_accuracy: 0.9094\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.3082 - accuracy: 0.9680 - val_loss: 0.7681 - val_accuracy: 0.8690\n",
      "Epoch 96/150\n",
      "30/34 [=========================>....] - ETA: 0s - loss: 0.2833 - accuracy: 0.9719INFO:tensorflow:Assets written to: model/eye_model\\model.96-0.93\\assets\n",
      "34/34 [==============================] - 5s 156ms/step - loss: 0.2806 - accuracy: 0.9737 - val_loss: 0.8964 - val_accuracy: 0.9339\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.2885 - accuracy: 0.9737 - val_loss: 1.0241 - val_accuracy: 0.9241\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.4232 - accuracy: 0.9182 - val_loss: 1.2355 - val_accuracy: 0.8274\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.3105 - accuracy: 0.9727 - val_loss: 0.8359 - val_accuracy: 0.9119\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2747 - accuracy: 0.9746 - val_loss: 0.8492 - val_accuracy: 0.9168\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3557 - accuracy: 0.9407 - val_loss: 0.7189 - val_accuracy: 0.9339\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2881 - accuracy: 0.9774 - val_loss: 0.8161 - val_accuracy: 0.9094\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2370 - accuracy: 0.9915 - val_loss: 0.8303 - val_accuracy: 0.9180\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.2267 - accuracy: 0.9934 - val_loss: 0.9448 - val_accuracy: 0.9302\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2405 - accuracy: 0.9821 - val_loss: 0.8331 - val_accuracy: 0.8788\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2437 - accuracy: 0.9821 - val_loss: 0.8167 - val_accuracy: 0.9155\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2987 - accuracy: 0.9614 - val_loss: 0.8963 - val_accuracy: 0.9082\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2638 - accuracy: 0.9774 - val_loss: 0.9275 - val_accuracy: 0.9070\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2425 - accuracy: 0.9831 - val_loss: 0.9925 - val_accuracy: 0.9058\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2455 - accuracy: 0.9812 - val_loss: 0.9248 - val_accuracy: 0.9094\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2101 - accuracy: 0.9897 - val_loss: 0.9331 - val_accuracy: 0.9143\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2007 - accuracy: 0.9925 - val_loss: 0.9264 - val_accuracy: 0.9327\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1947 - accuracy: 0.9934 - val_loss: 0.9406 - val_accuracy: 0.8898\n",
      "Epoch 114/150\n",
      "27/34 [======================>.......] - ETA: 0s - loss: 0.1883 - accuracy: 0.9965INFO:tensorflow:Assets written to: model/eye_model\\model.114-0.94\\assets\n",
      "34/34 [==============================] - 6s 195ms/step - loss: 0.1885 - accuracy: 0.9962 - val_loss: 0.9399 - val_accuracy: 0.9364\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2010 - accuracy: 0.9906 - val_loss: 1.2113 - val_accuracy: 0.9155\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.1947 - accuracy: 0.9915 - val_loss: 0.9791 - val_accuracy: 0.9339\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1804 - accuracy: 0.9972 - val_loss: 1.1227 - val_accuracy: 0.9315\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.3338 - accuracy: 0.9370 - val_loss: 0.6631 - val_accuracy: 0.9045\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3115 - accuracy: 0.9464 - val_loss: 0.7866 - val_accuracy: 0.8849\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.3158 - accuracy: 0.9464 - val_loss: 0.6051 - val_accuracy: 0.8960\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2094 - accuracy: 0.9915 - val_loss: 0.7299 - val_accuracy: 0.9217\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2379 - accuracy: 0.9727 - val_loss: 0.6154 - val_accuracy: 0.9302\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2148 - accuracy: 0.9849 - val_loss: 0.7012 - val_accuracy: 0.8531\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2269 - accuracy: 0.9737 - val_loss: 0.7877 - val_accuracy: 0.9119\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2962 - accuracy: 0.9501 - val_loss: 0.7157 - val_accuracy: 0.9094\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3024 - accuracy: 0.9501 - val_loss: 0.6736 - val_accuracy: 0.9192\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2128 - accuracy: 0.9831 - val_loss: 0.7419 - val_accuracy: 0.8715\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 1s 18ms/step - loss: 0.1897 - accuracy: 0.9887 - val_loss: 0.7420 - val_accuracy: 0.9302\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 0.1760 - accuracy: 0.9897 - val_loss: 0.8134 - val_accuracy: 0.9192\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1907 - accuracy: 0.9887 - val_loss: 0.8888 - val_accuracy: 0.9070\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1853 - accuracy: 0.9859 - val_loss: 1.0102 - val_accuracy: 0.9070\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2427 - accuracy: 0.9595 - val_loss: 0.7563 - val_accuracy: 0.9070\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1980 - accuracy: 0.9812 - val_loss: 0.7574 - val_accuracy: 0.9278\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1732 - accuracy: 0.9906 - val_loss: 0.8838 - val_accuracy: 0.9131\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1761 - accuracy: 0.9897 - val_loss: 1.1117 - val_accuracy: 0.8984\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2937 - accuracy: 0.9501 - val_loss: 0.6512 - val_accuracy: 0.9192\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2321 - accuracy: 0.9671 - val_loss: 0.5702 - val_accuracy: 0.9106\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1620 - accuracy: 0.9934 - val_loss: 0.7001 - val_accuracy: 0.9327\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1586 - accuracy: 0.9925 - val_loss: 0.8475 - val_accuracy: 0.9131\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 1s 18ms/step - loss: 0.1747 - accuracy: 0.9878 - val_loss: 0.7485 - val_accuracy: 0.9143\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1641 - accuracy: 0.9897 - val_loss: 0.7214 - val_accuracy: 0.8923\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1927 - accuracy: 0.9746 - val_loss: 0.7537 - val_accuracy: 0.9192\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1959 - accuracy: 0.9774 - val_loss: 0.9546 - val_accuracy: 0.8825\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2945 - accuracy: 0.9454 - val_loss: 0.6016 - val_accuracy: 0.9082\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1696 - accuracy: 0.9925 - val_loss: 0.7751 - val_accuracy: 0.9192\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1580 - accuracy: 0.9925 - val_loss: 0.7241 - val_accuracy: 0.9094\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2124 - accuracy: 0.9680 - val_loss: 0.5410 - val_accuracy: 0.9192\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1596 - accuracy: 0.9906 - val_loss: 0.7324 - val_accuracy: 0.9009\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1552 - accuracy: 0.9887 - val_loss: 0.7812 - val_accuracy: 0.9204\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1795 - accuracy: 0.9793 - val_loss: 0.7717 - val_accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(X_train,Y_train, epochs=150, batch_size=32, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
