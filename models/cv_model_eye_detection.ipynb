{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### work in this repository was inspired by \n",
    "##### https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe/blob/main/README_EN.md\n",
    "##### https://www.youtube.com/watch?v=a99p_fAr6e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# this is just to make sure that the results are reproducible to anyone that runs the code lol.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data formatting\n",
    "https://devqa.io/python-convert-csv-file-to-list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed\n",
      "['data/train/closed\\\\eye.csv', 'data/train/closed\\\\eye1.csv', 'data/train/closed\\\\eye2.csv']\n",
      "3\n",
      "[270.4326057434082, 254.2082405090332, 11.89279317855835, 316.92705154418945, 253.35582733154297, 7.74164617061615, 284.52354431152344, 257.31865882873535, 4.308481812477112, 301.39739990234375, 256.3886547088623, 3.0293849110603333, 300.3896903991699, 253.87916564941406, 1.3657981157302856, 283.58177185058594, 254.76920127868652, 2.539694309234619, 413.4335708618164, 261.1090564727783, 25.647451877593994, 372.53719329833984, 256.2609672546387, 13.838422298431396, 401.43199920654297, 262.17209815979004, 15.797088146209717, 386.1565399169922, 259.97328758239746, 11.58600926399231, 386.9802474975586, 257.97165870666504, 10.236743688583374, 402.0588684082031, 260.3184986114502, 14.386650323867798]\n",
      "training set:\n",
      "1990\n",
      "1990\n",
      "open\n",
      "['data/train/open\\\\eye.csv', 'data/train/open\\\\eye1.csv', 'data/train/open\\\\eye2.csv']\n",
      "3\n",
      "[240.9740447998047, 259.1934013366699, 6.445690989494324, 285.66261291503906, 265.6811714172363, 0.6067586690187454, 253.524169921875, 265.17674446105957, -1.1622554063796997, 269.8660469055176, 266.4568519592285, -3.0050191283226013, 269.06383514404297, 256.3341236114502, -6.1367493867874146, 252.04130172729492, 255.48151016235352, -4.117096960544586, 382.8523254394531, 267.01526641845703, 15.077458620071411, 342.9600143432617, 269.4540023803711, 4.455218315124512, 371.7563247680664, 272.9527759552002, 6.084950566291809, 357.14359283447266, 272.77347564697266, 2.3440393805503845, 358.8737487792969, 260.73869705200195, -0.5573234707117081, 374.16744232177734, 262.16614723205566, 3.309546113014221]\n",
      "training set:\n",
      "4475\n",
      "4475\n",
      "closed\n",
      "['data/test/closed\\\\eye.csv']\n",
      "1\n",
      "[262.9005241394043, 295.2817726135254, 3.9409154653549194, 302.97351837158203, 296.4185428619385, -6.174900531768799, 277.54432678222656, 299.472599029541, -3.3033421635627747, 292.52296447753906, 297.73109436035156, -6.945088505744934, 288.3000946044922, 296.1056327819824, -11.694833040237427, 272.7345275878906, 297.2181987762451, -7.654852271080017, 401.0587692260742, 295.25407791137695, -6.597258448600769, 359.37400817871094, 298.760404586792, -10.992343425750732, 386.1029815673828, 298.8584518432617, -12.129522562026978, 370.7989501953125, 298.6182975769043, -13.651512861251831, 374.4627380371094, 295.8701705932617, -18.604719638824463, 389.9569320678711, 296.2401008605957, -16.788511276245117]\n",
      "training set:\n",
      "413\n",
      "413\n",
      "open\n",
      "['data/test/open\\\\eye.csv']\n",
      "1\n",
      "[274.48841094970703, 173.11148643493652, 6.829060316085815, 336.1492919921875, 172.12753772735596, 3.620011806488037, 292.5924301147461, 178.40436458587646, -1.9173672795295715, 315.3747749328613, 177.21064567565918, -2.6635634899139404, 312.9568862915039, 172.13479042053223, -5.494605302810669, 290.3768539428711, 173.87499332427979, -4.651689231395721, 451.0616683959961, 176.24300479888916, 32.231318950653076, 402.16983795166016, 175.66766738891602, 14.981495141983032, 438.2963180541992, 181.65112495422363, 19.379394054412842, 419.80716705322266, 180.93938827514648, 13.260983228683472, 420.90728759765625, 172.09525108337402, 11.025840044021606, 439.32422637939453, 173.3092975616455, 17.284626960754395]\n",
      "training set:\n",
      "800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "labels = {0: \"closed\", 1: \"open\"}\n",
    "\n",
    "Y_train = [] #1d array, (big_number, 1)\n",
    "X_train = [] #2d array, (big_number, 42)\n",
    "def data_append_train(data, X_train = X_train, Y_train = Y_train):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/train\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_train.append([float(num) for num in row])\n",
    "                Y_train.append(label)\n",
    "    \n",
    "    print(X_train[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_train))\n",
    "    print(len(Y_train))    \n",
    "\n",
    "Y_test = []\n",
    "X_test = []\n",
    "def data_append_test(data, X_test = X_test, Y_test = Y_test):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/test\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_test.append([float(num) for num in row])\n",
    "                Y_test.append(label)\n",
    "    \n",
    "    print(X_test[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_test))\n",
    "    print(len(Y_test))    \n",
    "#i'll only use the train test.\n",
    "data = \"data\"\n",
    "data_append_train(data)\n",
    "data_append_test(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since I already have the training dataset split, I just need to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[242.33798981, 207.04376221,  23.31485987, ..., 429.30366516,\n",
       "         213.49546909, -10.80405235],\n",
       "        [ 86.97021484, 214.23394203,  42.55360126, ..., 208.25977325,\n",
       "         186.71218872,  -5.68372548],\n",
       "        [118.69736671, 216.20329857,  22.80077219, ..., 242.98435211,\n",
       "         193.09581757,  21.16004467],\n",
       "        ...,\n",
       "        [155.37166595, 202.89377689,  21.35821342, ..., 295.25913239,\n",
       "         171.39677525,  18.77847433],\n",
       "        [332.76046753,  98.79499912,   8.60808194, ..., 467.31437683,\n",
       "         101.90653324,  24.38389778],\n",
       "        [113.01543236,  92.34189749,  37.58370638, ..., 247.63631821,\n",
       "          75.5294466 ,   6.48359478]]),\n",
       " array([0, 1, 0, ..., 0, 1, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_df_train = pd.DataFrame(X_train)\n",
    "# Y_df_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# X_df_test = pd.DataFrame(X_test)\n",
    "# Y_df_test = pd.DataFrame(Y_test)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "# \n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# the random_state is so that both shuffle perform the same.\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_test,Y_test = shuffle(X_test, Y_test, random_state=0)\n",
    "# X_train.shape, Y_train.shape\n",
    "X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "# model_save_path = \"model/\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model/eye_model/model.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    #everytime the accuracy gets better, it saves\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (4475, 36)\n",
      "Shape of X_test: (800, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### since I am running binary classification with a sigmoid layer, the input shape is different than sparse categorical crossentropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "Y_train = np.asarray(Y_train).astype('float32').reshape((-1,1))\n",
    "Y_test = np.asarray(Y_test).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y_train: (4475, 1)\n",
      "Shape of Y_test: (800, 1)\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)\n",
    "\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    # I will just change the input \n",
    "    Dense(units=32, activation='relu', input_shape=(36,)),\n",
    "    Dense(units=32, activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.05)),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=16, activation='relu', \n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # since I am using sigmoid, the last layer has 1 unit, not two!\n",
    "    # Dense(units=1, activation = \"sigmoid\")\n",
    "\n",
    "    #okay, don't reuse this model\n",
    "    Dense(units = NUM_CLASSES, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#even though I should be using binary crossentropy... since two labels. but anyways.\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "140/140 [==============================] - ETA: 0s - loss: 8.6508 - accuracy: 0.5283INFO:tensorflow:Assets written to: model/eye_model\\model.01-0.30\\assets\n",
      "140/140 [==============================] - 11s 65ms/step - loss: 8.6508 - accuracy: 0.5283 - val_loss: 5.9073 - val_accuracy: 0.2975\n",
      "Epoch 2/150\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 4.6380 - accuracy: 0.5671INFO:tensorflow:Assets written to: model/eye_model\\model.02-0.42\\assets\n",
      "140/140 [==============================] - 6s 46ms/step - loss: 4.6051 - accuracy: 0.5672 - val_loss: 3.7085 - val_accuracy: 0.4175\n",
      "Epoch 3/150\n",
      "140/140 [==============================] - ETA: 0s - loss: 3.2451 - accuracy: 0.5718INFO:tensorflow:Assets written to: model/eye_model\\model.03-0.47\\assets\n",
      "140/140 [==============================] - 6s 43ms/step - loss: 3.2451 - accuracy: 0.5718 - val_loss: 2.9933 - val_accuracy: 0.4700\n",
      "Epoch 4/150\n",
      "139/140 [============================>.] - ETA: 0s - loss: 2.6836 - accuracy: 0.5973INFO:tensorflow:Assets written to: model/eye_model\\model.04-0.56\\assets\n",
      "140/140 [==============================] - 6s 42ms/step - loss: 2.6821 - accuracy: 0.5971 - val_loss: 2.5204 - val_accuracy: 0.5638\n",
      "Epoch 5/150\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 2.2766 - accuracy: 0.6130INFO:tensorflow:Assets written to: model/eye_model\\model.05-0.57\\assets\n",
      "140/140 [==============================] - 6s 41ms/step - loss: 2.2674 - accuracy: 0.6127 - val_loss: 2.1320 - val_accuracy: 0.5663\n",
      "Epoch 6/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.9430 - accuracy: 0.5996 - val_loss: 1.8519 - val_accuracy: 0.4988\n",
      "Epoch 7/150\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 1.6769 - accuracy: 0.6241INFO:tensorflow:Assets written to: model/eye_model\\model.07-0.57\\assets\n",
      "140/140 [==============================] - 6s 45ms/step - loss: 1.6723 - accuracy: 0.6219 - val_loss: 1.6495 - val_accuracy: 0.5738\n",
      "Epoch 8/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 1.4541 - accuracy: 0.6514 - val_loss: 1.4729 - val_accuracy: 0.5462\n",
      "Epoch 9/150\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 1.3082 - accuracy: 0.6488INFO:tensorflow:Assets written to: model/eye_model\\model.09-0.64\\assets\n",
      "140/140 [==============================] - 7s 47ms/step - loss: 1.3074 - accuracy: 0.6451 - val_loss: 1.2898 - val_accuracy: 0.6425\n",
      "Epoch 10/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.1778 - accuracy: 0.6653 - val_loss: 1.2037 - val_accuracy: 0.5525\n",
      "Epoch 11/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 1.0686 - accuracy: 0.6713 - val_loss: 1.1130 - val_accuracy: 0.5738\n",
      "Epoch 12/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.9758 - accuracy: 0.6963 - val_loss: 1.1141 - val_accuracy: 0.5763\n",
      "Epoch 13/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.9140 - accuracy: 0.6963 - val_loss: 1.0282 - val_accuracy: 0.6375\n",
      "Epoch 14/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.8635 - accuracy: 0.6992 - val_loss: 1.3429 - val_accuracy: 0.4588\n",
      "Epoch 15/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.8041 - accuracy: 0.7140 - val_loss: 0.9713 - val_accuracy: 0.5512\n",
      "Epoch 16/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.7404 - accuracy: 0.7323 - val_loss: 1.0262 - val_accuracy: 0.5537\n",
      "Epoch 17/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.7096 - accuracy: 0.7477 - val_loss: 0.8845 - val_accuracy: 0.5825\n",
      "Epoch 18/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.6763 - accuracy: 0.7526 - val_loss: 0.8743 - val_accuracy: 0.6175\n",
      "Epoch 19/150\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5983 - accuracy: 0.8078INFO:tensorflow:Assets written to: model/eye_model\\model.19-0.82\\assets\n",
      "140/140 [==============================] - 6s 43ms/step - loss: 0.5983 - accuracy: 0.8078 - val_loss: 0.6273 - val_accuracy: 0.8238\n",
      "Epoch 20/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.5331 - accuracy: 0.8407 - val_loss: 0.5600 - val_accuracy: 0.8175\n",
      "Epoch 21/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.4715 - accuracy: 0.8715 - val_loss: 0.4821 - val_accuracy: 0.8238\n",
      "Epoch 22/150\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3897 - accuracy: 0.9031INFO:tensorflow:Assets written to: model/eye_model\\model.22-0.90\\assets\n",
      "140/140 [==============================] - 7s 53ms/step - loss: 0.3883 - accuracy: 0.9037 - val_loss: 0.3772 - val_accuracy: 0.9013\n",
      "Epoch 23/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.3200 - accuracy: 0.9323 - val_loss: 0.5771 - val_accuracy: 0.8112\n",
      "Epoch 24/150\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.9300INFO:tensorflow:Assets written to: model/eye_model\\model.24-0.95\\assets\n",
      "140/140 [==============================] - 6s 40ms/step - loss: 0.3211 - accuracy: 0.9303 - val_loss: 0.2555 - val_accuracy: 0.9525\n",
      "Epoch 25/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2690 - accuracy: 0.9473 - val_loss: 0.2862 - val_accuracy: 0.9438\n",
      "Epoch 26/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2319 - accuracy: 0.9611 - val_loss: 0.3012 - val_accuracy: 0.9162\n",
      "Epoch 27/150\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2781 - accuracy: 0.9380INFO:tensorflow:Assets written to: model/eye_model\\model.27-0.96\\assets\n",
      "140/140 [==============================] - 6s 40ms/step - loss: 0.2765 - accuracy: 0.9388 - val_loss: 0.2371 - val_accuracy: 0.9563\n",
      "Epoch 28/150\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2376 - accuracy: 0.9572INFO:tensorflow:Assets written to: model/eye_model\\model.28-0.97\\assets\n",
      "140/140 [==============================] - 6s 43ms/step - loss: 0.2366 - accuracy: 0.9578 - val_loss: 0.1986 - val_accuracy: 0.9712\n",
      "Epoch 29/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2322 - accuracy: 0.9560 - val_loss: 0.3757 - val_accuracy: 0.9162\n",
      "Epoch 30/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2050 - accuracy: 0.9645 - val_loss: 0.2716 - val_accuracy: 0.9550\n",
      "Epoch 31/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3261 - accuracy: 0.9137 - val_loss: 0.5563 - val_accuracy: 0.8562\n",
      "Epoch 32/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.2235 - accuracy: 0.9551 - val_loss: 0.3730 - val_accuracy: 0.9187\n",
      "Epoch 33/150\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9653INFO:tensorflow:Assets written to: model/eye_model\\model.33-0.98\\assets\n",
      "140/140 [==============================] - 6s 43ms/step - loss: 0.1939 - accuracy: 0.9649 - val_loss: 0.1721 - val_accuracy: 0.9787\n",
      "Epoch 34/150\n",
      "140/140 [==============================] - 2s 13ms/step - loss: 0.2508 - accuracy: 0.9450 - val_loss: 0.2378 - val_accuracy: 0.9588\n",
      "Epoch 35/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.2485 - accuracy: 0.9499 - val_loss: 0.2244 - val_accuracy: 0.9613\n",
      "Epoch 36/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1697 - accuracy: 0.9732 - val_loss: 0.2492 - val_accuracy: 0.9575\n",
      "Epoch 37/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1963 - accuracy: 0.9627 - val_loss: 0.2339 - val_accuracy: 0.9475\n",
      "Epoch 38/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.2530 - accuracy: 0.9363 - val_loss: 0.1991 - val_accuracy: 0.9613\n",
      "Epoch 39/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2226 - accuracy: 0.9497 - val_loss: 0.1932 - val_accuracy: 0.9750\n",
      "Epoch 40/150\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2153 - accuracy: 0.9527INFO:tensorflow:Assets written to: model/eye_model\\model.40-0.99\\assets\n",
      "140/140 [==============================] - 6s 43ms/step - loss: 0.2149 - accuracy: 0.9528 - val_loss: 0.1401 - val_accuracy: 0.9912\n",
      "Epoch 41/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1881 - accuracy: 0.9645 - val_loss: 0.2259 - val_accuracy: 0.9613\n",
      "Epoch 42/150\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.1957 - accuracy: 0.9625 - val_loss: 0.3005 - val_accuracy: 0.9337\n",
      "Epoch 43/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1751 - accuracy: 0.9694 - val_loss: 0.1589 - val_accuracy: 0.9787\n",
      "Epoch 44/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1872 - accuracy: 0.9625 - val_loss: 0.2200 - val_accuracy: 0.9600\n",
      "Epoch 45/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2016 - accuracy: 0.9575 - val_loss: 0.1684 - val_accuracy: 0.9638\n",
      "Epoch 46/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3230 - accuracy: 0.9064 - val_loss: 0.3070 - val_accuracy: 0.9488\n",
      "Epoch 47/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1793 - accuracy: 0.9665 - val_loss: 0.1588 - val_accuracy: 0.9650\n",
      "Epoch 48/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2328 - accuracy: 0.9441 - val_loss: 0.1874 - val_accuracy: 0.9600\n",
      "Epoch 49/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1911 - accuracy: 0.9600 - val_loss: 0.2633 - val_accuracy: 0.9638\n",
      "Epoch 50/150\n",
      "140/140 [==============================] - 2s 11ms/step - loss: 0.1715 - accuracy: 0.9696 - val_loss: 0.3746 - val_accuracy: 0.9237\n",
      "Epoch 51/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2121 - accuracy: 0.9526 - val_loss: 0.6679 - val_accuracy: 0.7225\n",
      "Epoch 52/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1880 - accuracy: 0.9629 - val_loss: 0.1949 - val_accuracy: 0.9663\n",
      "Epoch 53/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1580 - accuracy: 0.9707 - val_loss: 0.2233 - val_accuracy: 0.9663\n",
      "Epoch 54/150\n",
      "140/140 [==============================] - 1s 11ms/step - loss: 0.2501 - accuracy: 0.9272 - val_loss: 0.3172 - val_accuracy: 0.8737\n",
      "Epoch 55/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1650 - accuracy: 0.9696 - val_loss: 0.1190 - val_accuracy: 0.9850\n",
      "Epoch 56/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2444 - accuracy: 0.9316 - val_loss: 0.1546 - val_accuracy: 0.9800\n",
      "Epoch 57/150\n",
      "140/140 [==============================] - 1s 11ms/step - loss: 0.1548 - accuracy: 0.9718 - val_loss: 0.1295 - val_accuracy: 0.9900\n",
      "Epoch 58/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.2626 - accuracy: 0.9207 - val_loss: 0.1531 - val_accuracy: 0.9650\n",
      "Epoch 59/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1773 - accuracy: 0.9607 - val_loss: 0.3407 - val_accuracy: 0.9187\n",
      "Epoch 60/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1494 - accuracy: 0.9707 - val_loss: 0.2411 - val_accuracy: 0.9475\n",
      "Epoch 61/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1930 - accuracy: 0.9533 - val_loss: 0.2822 - val_accuracy: 0.9312\n",
      "Epoch 62/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2240 - accuracy: 0.9428 - val_loss: 0.2846 - val_accuracy: 0.9575\n",
      "Epoch 63/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1482 - accuracy: 0.9705 - val_loss: 0.2462 - val_accuracy: 0.9613\n",
      "Epoch 64/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1538 - accuracy: 0.9685 - val_loss: 0.3575 - val_accuracy: 0.9112\n",
      "Epoch 65/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2091 - accuracy: 0.9486 - val_loss: 0.1839 - val_accuracy: 0.9650\n",
      "Epoch 66/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1668 - accuracy: 0.9665 - val_loss: 0.3089 - val_accuracy: 0.9563\n",
      "Epoch 67/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1568 - accuracy: 0.9651 - val_loss: 0.4121 - val_accuracy: 0.9150\n",
      "Epoch 68/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1571 - accuracy: 0.9663 - val_loss: 0.1697 - val_accuracy: 0.9625\n",
      "Epoch 69/150\n",
      "140/140 [==============================] - 2s 14ms/step - loss: 0.1571 - accuracy: 0.9669 - val_loss: 0.1351 - val_accuracy: 0.9638\n",
      "Epoch 70/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1522 - accuracy: 0.9665 - val_loss: 0.2507 - val_accuracy: 0.9488\n",
      "Epoch 71/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1511 - accuracy: 0.9663 - val_loss: 0.4280 - val_accuracy: 0.9513\n",
      "Epoch 72/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1506 - accuracy: 0.9694 - val_loss: 0.4534 - val_accuracy: 0.8875\n",
      "Epoch 73/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1626 - accuracy: 0.9629 - val_loss: 0.1255 - val_accuracy: 0.9800\n",
      "Epoch 74/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1595 - accuracy: 0.9638 - val_loss: 0.1411 - val_accuracy: 0.9638\n",
      "Epoch 75/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1834 - accuracy: 0.9495 - val_loss: 0.1913 - val_accuracy: 0.9712\n",
      "Epoch 76/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1451 - accuracy: 0.9698 - val_loss: 0.2767 - val_accuracy: 0.9613\n",
      "Epoch 77/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1491 - accuracy: 0.9663 - val_loss: 0.1401 - val_accuracy: 0.9650\n",
      "Epoch 78/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1503 - accuracy: 0.9663 - val_loss: 0.1435 - val_accuracy: 0.9625\n",
      "Epoch 79/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1676 - accuracy: 0.9562 - val_loss: 0.1667 - val_accuracy: 0.9775\n",
      "Epoch 80/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1699 - accuracy: 0.9542 - val_loss: 0.1839 - val_accuracy: 0.9600\n",
      "Epoch 81/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1461 - accuracy: 0.9676 - val_loss: 0.3052 - val_accuracy: 0.9613\n",
      "Epoch 82/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.2189 - accuracy: 0.9347 - val_loss: 0.3840 - val_accuracy: 0.9200\n",
      "Epoch 83/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1341 - accuracy: 0.9734 - val_loss: 0.1342 - val_accuracy: 0.9638\n",
      "Epoch 84/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1707 - accuracy: 0.9575 - val_loss: 0.3542 - val_accuracy: 0.8625\n",
      "Epoch 85/150\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.1374 - accuracy: 0.9701 - val_loss: 0.1745 - val_accuracy: 0.9750\n",
      "Epoch 86/150\n",
      "140/140 [==============================] - 1s 11ms/step - loss: 0.1671 - accuracy: 0.9582 - val_loss: 0.1303 - val_accuracy: 0.9688\n",
      "Epoch 87/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1717 - accuracy: 0.9520 - val_loss: 0.1164 - val_accuracy: 0.9812\n",
      "Epoch 88/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1578 - accuracy: 0.9591 - val_loss: 0.1554 - val_accuracy: 0.9613\n",
      "Epoch 89/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1356 - accuracy: 0.9687 - val_loss: 0.3319 - val_accuracy: 0.9300\n",
      "Epoch 90/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1345 - accuracy: 0.9672 - val_loss: 0.1123 - val_accuracy: 0.9725\n",
      "Epoch 91/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1272 - accuracy: 0.9707 - val_loss: 0.1231 - val_accuracy: 0.9725\n",
      "Epoch 92/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1384 - accuracy: 0.9705 - val_loss: 0.4881 - val_accuracy: 0.8125\n",
      "Epoch 93/150\n",
      "140/140 [==============================] - 2s 11ms/step - loss: 0.2554 - accuracy: 0.9001 - val_loss: 0.1237 - val_accuracy: 0.9787\n",
      "Epoch 94/150\n",
      "140/140 [==============================] - 2s 11ms/step - loss: 0.1304 - accuracy: 0.9723 - val_loss: 0.3666 - val_accuracy: 0.9275\n",
      "Epoch 95/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1454 - accuracy: 0.9669 - val_loss: 0.1241 - val_accuracy: 0.9725\n",
      "Epoch 96/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1415 - accuracy: 0.9656 - val_loss: 0.1650 - val_accuracy: 0.9500\n",
      "Epoch 97/150\n",
      "140/140 [==============================] - 1s 11ms/step - loss: 0.2593 - accuracy: 0.9258 - val_loss: 0.4073 - val_accuracy: 0.9513\n",
      "Epoch 98/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2048 - accuracy: 0.9676 - val_loss: 0.1710 - val_accuracy: 0.9650\n",
      "Epoch 99/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1612 - accuracy: 0.9687 - val_loss: 0.1436 - val_accuracy: 0.9675\n",
      "Epoch 100/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1631 - accuracy: 0.9598 - val_loss: 0.2438 - val_accuracy: 0.9413\n",
      "Epoch 101/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1536 - accuracy: 0.9634 - val_loss: 0.3876 - val_accuracy: 0.8625\n",
      "Epoch 102/150\n",
      "140/140 [==============================] - 2s 14ms/step - loss: 0.1411 - accuracy: 0.9696 - val_loss: 0.3442 - val_accuracy: 0.9187\n",
      "Epoch 103/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2364 - accuracy: 0.9274 - val_loss: 0.1118 - val_accuracy: 0.9837\n",
      "Epoch 104/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1333 - accuracy: 0.9707 - val_loss: 0.2328 - val_accuracy: 0.9413\n",
      "Epoch 105/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.3853 - accuracy: 0.8351 - val_loss: 0.3020 - val_accuracy: 0.9563\n",
      "Epoch 106/150\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.1545 - accuracy: 0.9721 - val_loss: 0.1949 - val_accuracy: 0.9675\n",
      "Epoch 107/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1284 - accuracy: 0.9734 - val_loss: 0.1941 - val_accuracy: 0.9513\n",
      "Epoch 108/150\n",
      "140/140 [==============================] - 2s 14ms/step - loss: 0.1912 - accuracy: 0.9479 - val_loss: 0.1444 - val_accuracy: 0.9812\n",
      "Epoch 109/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1365 - accuracy: 0.9687 - val_loss: 0.1137 - val_accuracy: 0.9825\n",
      "Epoch 110/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1488 - accuracy: 0.9631 - val_loss: 0.1713 - val_accuracy: 0.9450\n",
      "Epoch 111/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1409 - accuracy: 0.9667 - val_loss: 0.6357 - val_accuracy: 0.7650\n",
      "Epoch 112/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1549 - accuracy: 0.9613 - val_loss: 0.2458 - val_accuracy: 0.9613\n",
      "Epoch 113/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1559 - accuracy: 0.9573 - val_loss: 0.1358 - val_accuracy: 0.9700\n",
      "Epoch 114/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1321 - accuracy: 0.9696 - val_loss: 0.1210 - val_accuracy: 0.9675\n",
      "Epoch 115/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1327 - accuracy: 0.9645 - val_loss: 0.3058 - val_accuracy: 0.9300\n",
      "Epoch 116/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1195 - accuracy: 0.9730 - val_loss: 0.1552 - val_accuracy: 0.9613\n",
      "Epoch 117/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1796 - accuracy: 0.9522 - val_loss: 0.1082 - val_accuracy: 0.9837\n",
      "Epoch 118/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1298 - accuracy: 0.9709 - val_loss: 0.1244 - val_accuracy: 0.9787\n",
      "Epoch 119/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1347 - accuracy: 0.9676 - val_loss: 0.0935 - val_accuracy: 0.9837\n",
      "Epoch 120/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2567 - accuracy: 0.8907 - val_loss: 0.8414 - val_accuracy: 0.5725\n",
      "Epoch 121/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3844 - accuracy: 0.8498 - val_loss: 0.1617 - val_accuracy: 0.9675\n",
      "Epoch 122/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1747 - accuracy: 0.9625 - val_loss: 0.1598 - val_accuracy: 0.9588\n",
      "Epoch 123/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1589 - accuracy: 0.9625 - val_loss: 0.1268 - val_accuracy: 0.9663\n",
      "Epoch 124/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1563 - accuracy: 0.9672 - val_loss: 0.1221 - val_accuracy: 0.9712\n",
      "Epoch 125/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1534 - accuracy: 0.9651 - val_loss: 0.1921 - val_accuracy: 0.9500\n",
      "Epoch 126/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1364 - accuracy: 0.9707 - val_loss: 0.2507 - val_accuracy: 0.9400\n",
      "Epoch 127/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.2154 - accuracy: 0.9267 - val_loss: 0.1197 - val_accuracy: 0.9712\n",
      "Epoch 128/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1358 - accuracy: 0.9743 - val_loss: 0.1866 - val_accuracy: 0.9700\n",
      "Epoch 129/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1470 - accuracy: 0.9687 - val_loss: 0.1843 - val_accuracy: 0.9538\n",
      "Epoch 130/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1765 - accuracy: 0.9549 - val_loss: 0.2794 - val_accuracy: 0.9237\n",
      "Epoch 131/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1746 - accuracy: 0.9528 - val_loss: 0.1911 - val_accuracy: 0.9775\n",
      "Epoch 132/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1551 - accuracy: 0.9676 - val_loss: 0.0840 - val_accuracy: 0.9862\n",
      "Epoch 133/150\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9703INFO:tensorflow:Assets written to: model/eye_model\\model.133-0.99\\assets\n",
      "140/140 [==============================] - 10s 71ms/step - loss: 0.1336 - accuracy: 0.9707 - val_loss: 0.0790 - val_accuracy: 0.9925\n",
      "Epoch 134/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1729 - accuracy: 0.9526 - val_loss: 0.1875 - val_accuracy: 0.9700\n",
      "Epoch 135/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1946 - accuracy: 0.9439 - val_loss: 0.1593 - val_accuracy: 0.9550\n",
      "Epoch 136/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1237 - accuracy: 0.9721 - val_loss: 0.1555 - val_accuracy: 0.9475\n",
      "Epoch 137/150\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.1376 - accuracy: 0.9683 - val_loss: 0.1112 - val_accuracy: 0.9787\n",
      "Epoch 138/150\n",
      "140/140 [==============================] - 2s 11ms/step - loss: 0.1306 - accuracy: 0.9705 - val_loss: 0.1295 - val_accuracy: 0.9575\n",
      "Epoch 139/150\n",
      "140/140 [==============================] - 2s 15ms/step - loss: 0.1397 - accuracy: 0.9674 - val_loss: 0.1256 - val_accuracy: 0.9588\n",
      "Epoch 140/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1421 - accuracy: 0.9616 - val_loss: 0.1500 - val_accuracy: 0.9588\n",
      "Epoch 141/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1319 - accuracy: 0.9658 - val_loss: 0.1093 - val_accuracy: 0.9638\n",
      "Epoch 142/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1524 - accuracy: 0.9571 - val_loss: 0.1200 - val_accuracy: 0.9762\n",
      "Epoch 143/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1436 - accuracy: 0.9667 - val_loss: 0.2232 - val_accuracy: 0.9438\n",
      "Epoch 144/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1258 - accuracy: 0.9709 - val_loss: 0.1196 - val_accuracy: 0.9600\n",
      "Epoch 145/150\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.1293 - accuracy: 0.9663 - val_loss: 0.1894 - val_accuracy: 0.9525\n",
      "Epoch 146/150\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1625 - accuracy: 0.9495 - val_loss: 0.1060 - val_accuracy: 0.9812\n",
      "Epoch 147/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1214 - accuracy: 0.9705 - val_loss: 0.1172 - val_accuracy: 0.9775\n",
      "Epoch 148/150\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1557 - accuracy: 0.9562 - val_loss: 0.4507 - val_accuracy: 0.8913\n",
      "Epoch 149/150\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1604 - accuracy: 0.9596 - val_loss: 0.1096 - val_accuracy: 0.9787\n",
      "Epoch 150/150\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1358 - accuracy: 0.9638 - val_loss: 0.1803 - val_accuracy: 0.9550\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(X_train,Y_train, epochs=150, batch_size=32, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
