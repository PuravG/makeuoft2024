{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### work in this repository was inspired by \n",
    "##### https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe/blob/main/README_EN.md\n",
    "##### https://www.youtube.com/watch?v=a99p_fAr6e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# this is just to make sure that the results are reproducible to anyone that runs the code lol.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data formatting\n",
    "https://devqa.io/python-convert-csv-file-to-list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_yawn\n",
      "['data/train/no_yawn\\\\eye.csv']\n",
      "1\n",
      "[335.7875442504883, 307.1158504486084, -47.087249755859375, 336.71512603759766, 296.57092094421387, -51.64748668670654, 336.2636947631836, 280.96996307373047, -49.989943504333496, 333.3477020263672, 241.02779388427734, -31.407363414764404, 332.73876190185547, 212.79132843017578, -29.069066047668457, 327.1074676513672, 345.0590229034424, -8.09687077999115, 327.3999786376953, 306.43741607666016, -47.44572639465332, 272.0389938354492, 340.79870223999023, 4.277186393737793, 213.45247268676758, 321.92012786865234, 75.54635047912598, 306.8148422241211, 304.96450424194336, -23.216757774353027, 298.4514808654785, 302.03410148620605, -22.908165454864502, 290.2219581604004, 345.4917812347412, -0.9893196076154709, 317.0164680480957, 344.7213363647461, -9.153054356575012, 299.5151901245117, 343.9896011352539, -4.636331796646118, 298.1235122680664, 345.52124977111816, -5.400487184524536, 202.47108459472656, 267.89669036865234, 77.93771266937256, 232.5141143798828, 212.59323120117188, -7.217577695846558, 206.30939483642578, 294.1915798187256, 78.99801731109619, 238.3456039428711, 363.0993175506592, 52.45786190032959, 299.7085952758789, 401.88014030456543, 23.53867769241333, 268.9757537841797, 388.3469009399414, 34.93741273880005, 255.0728988647461, 378.3435344696045, 42.46098518371582, 319.2539978027344, 403.0469512939453, 24.407994747161865, 332.3814392089844, 226.04641914367676, -26.816940307617188, 224.66323852539062, 345.1955509185791, 65.03156661987305, 283.6514091491699, 396.68128967285156, 27.13603973388672, 307.64760971069336, 344.24211502075195, -7.519798278808594, 301.4065742492676, 222.87506103515625, -10.13853669166565, 335.2461624145508, 267.323055267334, -43.459177017211914, 334.30992126464844, 254.63361740112305, -37.30515003204346, 282.0430374145508, 205.63676834106445, -22.530736923217773, 253.7395477294922, 204.6492576599121, -22.1132755279541, 201.16939544677734, 243.60194206237793, 72.48523712158203, 397.7121353149414, 323.75658988952637, 110.95830917358398, 360.11817932128906, 302.8629398345947, -12.987537384033203, 355.9553909301758, 343.4336757659912, 11.799042224884033, 335.7073974609375, 344.30920600891113, -4.985620081424713, 350.130615234375, 344.0119171142578, 4.543665945529938, 409.31495666503906, 273.2057189941406, 117.2843074798584, 404.6417999267578, 297.87586212158203, 116.69163703918457, 380.47374725341797, 361.5732479095459, 79.1675615310669, 415.20896911621094, 276.1669921875, 83.85224342346191, 395.5793762207031, 338.90247344970703, 72.92778015136719, 336.49314880371094, 400.5127143859863, 30.334641933441162, 358.9384078979492, 385.53351402282715, 52.15827465057373, 368.7136459350586, 375.69562911987305, 64.1606855392456, 389.61475372314453, 345.2706241607666, 96.66378974914551, 348.84864807128906, 394.28112030029297, 39.787843227386475, 410.3373718261719, 298.4429454803467, 85.70389747619629, 342.84820556640625, 343.31048011779785, -0.10001461952924728, 349.1668701171875, 223.85709285736084, -4.289259612560272, 404.3389892578125, 319.1659355163574, 84.63764190673828, 356.6443634033203, 215.13510704040527, -7.079983949661255, 371.2701416015625, 209.24853801727295, -7.215548157691956, 383.74542236328125, 208.42577934265137, -4.190442264080048, 399.0354919433594, 214.77795124053955, 9.63923454284668, 364.9058532714844, 246.33195877075195, 4.881795644760132, 349.12418365478516, 304.4809341430664, -39.81073617935181]\n",
      "training set:\n",
      "691\n",
      "691\n",
      "yawn\n",
      "['data/train/yawn\\\\eye.csv']\n",
      "1\n",
      "[332.0860290527344, 285.83810806274414, -46.802334785461426, 333.23360443115234, 276.6072463989258, -48.33548545837402, 333.18359375, 265.7161045074463, -43.577208518981934, 331.76502227783203, 239.8310422897339, -17.272930145263672, 332.54859924316406, 218.311185836792, -8.429931402206421, 319.4565773010254, 360.1690864562988, -15.18399953842163, 325.29754638671875, 285.0042915344238, -47.17535972595215, 272.81442642211914, 330.57887077331543, -10.304266214370728, 226.5472412109375, 327.4302577972412, 44.99471187591553, 307.2308540344238, 287.8088092803955, -27.264630794525146, 300.07274627685547, 286.4004421234131, -27.398147583007812, 289.4363212585449, 348.729829788208, -12.106659412384033, 310.5224800109863, 358.4278106689453, -16.302478313446045, 297.8863525390625, 351.67797088623047, -13.017641305923462, 295.8053398132324, 352.63209342956543, -13.945356607437134, 220.3584098815918, 279.7250461578369, 60.31275749206543, 252.23344802856445, 219.80445384979248, 5.773228406906128, 221.47321701049805, 302.07077980041504, 54.559125900268555, 243.60610961914062, 364.1575813293457, 22.21909999847412, 291.90317153930664, 405.9071445465088, 0.6072770059108734, 265.1380729675293, 388.5487747192383, 9.823126792907715, 254.82343673706055, 378.2427406311035, 15.192021131515503, 310.726375579834, 409.6189212799072, 2.121269851922989, 331.6013717651367, 229.2978000640869, -9.646400809288025, 234.72095489501953, 348.7057685852051, 32.62336015701294, 277.2492980957031, 398.236141204834, 3.683518171310425, 303.42308044433594, 355.32508850097656, -15.221004486083984, 307.18761444091797, 228.1900119781494, 3.5308629274368286, 332.6068115234375, 257.028751373291, -34.4807243347168, 332.12833404541016, 248.8516330718994, -25.336320400238037, 293.92147064208984, 211.51325225830078, -3.748910427093506, 271.67442321777344, 209.9805450439453, -5.1083290576934814, 221.21770858764648, 259.4407653808594, 62.68573760986328, 383.5638427734375, 341.91049575805664, 79.51807022094727, 352.15606689453125, 292.2131824493408, -17.25484013557434, 343.8728332519531, 352.0708465576172, 1.9247418642044067, 327.36087799072266, 359.43846702575684, -11.637701988220215, 338.79589080810547, 355.13431549072266, -3.072637915611267, 395.8058166503906, 298.74037742614746, 98.44877243041992, 391.6065979003906, 319.3162536621094, 90.98554611206055, 367.1892547607422, 372.6164245605469, 48.255348205566406, 401.53228759765625, 295.3055763244629, 71.38031959533691, 378.83140563964844, 349.20727729797363, 48.06673526763916, 328.30406188964844, 407.794246673584, 7.645280957221985, 350.3706359863281, 393.0626106262207, 26.81645393371582, 358.35826873779297, 384.21692848205566, 36.45317792892456, 374.8340606689453, 359.92956161499023, 63.32479476928711, 340.85304260253906, 401.3921070098877, 16.30820393562317, 396.6307830810547, 314.73647117614746, 66.65255069732666, 332.8527069091797, 356.89172744750977, -7.0756131410598755, 345.56602478027344, 231.3717269897461, 9.948700070381165, 389.0623474121094, 332.9036521911621, 60.41724681854248, 352.2883987426758, 224.1767120361328, 10.343337059020996, 364.9039840698242, 220.17823219299316, 11.80653691291809, 375.6219482421875, 220.29627799987793, 14.682966470718384, 388.91212463378906, 228.3617877960205, 25.77221393585205, 358.9974594116211, 253.68304252624512, 12.447172403335571, 342.82936096191406, 286.48037910461426, -39.58946943283081]\n",
      "training set:\n",
      "990\n",
      "990\n",
      "no_yawn\n",
      "['data/test/no_yawn\\\\eye.csv']\n",
      "1\n",
      "[482.1818161010742, 261.08842849731445, -51.26882553100586, 482.83275604248047, 249.94465827941895, -54.807071685791016, 483.38367462158203, 235.01448154449463, -51.23660087585449, 484.45674896240234, 198.73334884643555, -25.948598384857178, 485.3153991699219, 171.48001670837402, -20.120704174041748, 480.4308319091797, 307.540225982666, -17.005282640457153, 474.0159225463867, 260.39076805114746, -50.500359535217285, 427.48058319091797, 306.4533805847168, 4.116925001144409, 380.0724411010742, 297.01257705688477, 89.64959144592285, 457.6869583129883, 262.29397773742676, -22.974705696105957, 449.66480255126953, 260.500431060791, -21.37593984603882, 445.85205078125, 308.9824962615967, -4.639378786087036, 470.9140396118164, 306.6961669921875, -16.00742816925049, 454.92103576660156, 306.23897552490234, -8.741504549980164, 453.2466506958008, 308.2125663757324, -10.058101415634155, 371.52099609375, 242.69737243652344, 102.5670337677002, 392.9051208496094, 177.0439910888672, 17.373478412628174, 374.07745361328125, 269.27770614624023, 98.79946708679199, 400.6540298461914, 335.49951553344727, 56.20986461639404, 454.6930694580078, 373.35153579711914, 12.450660467147827, 426.82857513427734, 359.1982841491699, 31.031458377838135, 414.7999572753906, 349.6841812133789, 41.912851333618164, 474.4437026977539, 375.8544731140137, 9.950668811798096, 484.8219680786133, 184.99613285064697, -19.267162084579468, 389.527587890625, 319.2287063598633, 73.86888980865479, 439.65599060058594, 367.44375228881836, 19.788854122161865, 462.2883987426758, 306.1010456085205, -12.81043529510498, 458.6590576171875, 185.2136993408203, 3.0586177110671997, 483.7603759765625, 222.54953384399414, -42.45612144470215, 484.0934371948242, 211.00167274475098, -33.8797402381897, 439.46533203125, 166.8350887298584, -5.981004238128662, 412.0964813232422, 166.44115447998047, -1.4864523708820343, 371.01165771484375, 218.06425094604492, 101.25782012939453, 574.9592208862305, 304.91031646728516, 92.35588073730469, 512.6219177246094, 263.2585144042969, -20.6081223487854, 511.0020065307617, 312.1394348144531, -3.5308045148849487, 489.2620086669922, 307.6258850097656, -15.687342882156372, 504.60121154785156, 310.67235946655273, -9.185864925384521, 588.2770538330078, 251.7798614501953, 105.49103736877441, 583.4561157226562, 277.972412109375, 101.6860580444336, 551.0050201416016, 341.15123748779297, 58.265438079833984, 588.7511825561523, 250.56607246398926, 70.08880138397217, 565.5774307250977, 315.9694290161133, 52.42556571960449, 494.1598129272461, 374.8101997375488, 13.060853481292725, 522.761344909668, 362.5660514831543, 32.37419366836548, 535.6909561157227, 354.0429496765137, 43.64744186401367, 563.6018753051758, 325.962610244751, 76.27861976623535, 509.3918228149414, 369.91418838500977, 20.774354934692383, 583.8970947265625, 274.4667720794678, 69.1646957397461, 496.8375015258789, 307.76206970214844, -12.174301147460938, 506.9560241699219, 185.57650566101074, 0.08226437494158745, 577.1153259277344, 296.30693435668945, 65.35274505615234, 515.2457046508789, 176.67780876159668, -3.0625781416893005, 531.3644027709961, 170.76534748077393, -4.68711256980896, 545.3300094604492, 170.00800609588623, -3.789830207824707, 564.2354583740234, 177.68553256988525, 6.796239614486694, 524.1601181030273, 210.26957988739014, 2.6640883088111877, 497.05284118652344, 259.6634101867676, -45.5497407913208]\n",
      "training set:\n",
      "338\n",
      "338\n",
      "yawn\n",
      "['data/test/yawn\\\\eye.csv']\n",
      "1\n",
      "[242.79800415039062, 290.7513999938965, -53.21986675262451, 242.30403900146484, 279.64213371276855, -57.307701110839844, 240.27631759643555, 264.4527339935303, -54.551873207092285, 233.8381004333496, 226.6416835784912, -31.77013874053955, 229.99088287353516, 197.7088737487793, -27.04880714416504, 243.59111785888672, 360.2245903015137, -6.451576352119446, 234.3002700805664, 291.1380672454834, -53.337745666503906, 183.8692855834961, 339.65749740600586, 6.001219153404236, 128.28760147094727, 330.386438369751, 76.538405418396, 214.0602684020996, 293.4077453613281, -27.530362606048584, 204.93751525878906, 291.663293838501, -26.98728084564209, 202.58432388305664, 352.9490089416504, 2.031026929616928, 231.9432258605957, 360.8141899108887, -6.931779980659485, 213.34226608276367, 355.68400382995605, -1.4682179689407349, 211.46602630615234, 357.0182991027832, -2.056663930416107, 109.02485847473145, 275.577449798584, 82.51452445983887, 129.2885398864746, 211.49714469909668, -1.2447410821914673, 116.42220497131348, 301.9485855102539, 81.74736976623535, 158.3470344543457, 371.00830078125, 53.48684310913086, 222.86188125610352, 412.0220947265625, 27.341148853302002, 190.10719299316406, 396.36380195617676, 38.33338975906372, 175.7695770263672, 385.8489418029785, 44.88492488861084, 243.84933471679688, 412.46386528015137, 27.932004928588867, 231.4075469970703, 211.69238090515137, -25.773210525512695, 143.16740036010742, 353.86834144592285, 64.9931812286377, 205.4764175415039, 405.88937759399414, 31.04893207550049, 221.80278778076172, 359.132022857666, -4.844290614128113, 201.27607345581055, 213.6429262161255, -6.983250379562378, 238.04996490478516, 251.67054176330566, -46.56522274017334, 235.99685668945312, 239.7601318359375, -38.84204149246216, 178.12795639038086, 197.26407051086426, -18.059674501419067, 148.89599800109863, 199.50722694396973, -16.80443048477173, 104.62664604187012, 251.12537384033203, 78.84582042694092, 317.48363494873047, 313.0686664581299, 105.28581619262695, 269.6544647216797, 285.5567264556885, -18.74048113822937, 270.17162322998047, 342.6390266418457, 12.775959968566895, 252.7627182006836, 356.86503410339355, -3.3718979358673096, 264.99019622802734, 347.94633865356445, 6.358324289321899, 323.5367202758789, 258.1827449798584, 114.11618232727051, 322.21435546875, 284.2411136627197, 112.1103572845459, 302.8651809692383, 355.71081161499023, 75.15557289123535, 329.5993423461914, 258.4933376312256, 79.48703289031982, 314.74720001220703, 328.3490753173828, 68.1666374206543, 262.32418060302734, 406.93161964416504, 32.9796028137207, 284.3669891357422, 384.96336936950684, 52.40516662597656, 292.9559898376465, 372.3972702026367, 62.53363609313965, 310.58874130249023, 337.44489669799805, 90.76019287109375, 274.9391174316406, 397.07940101623535, 41.401591300964355, 327.8586196899414, 282.7152442932129, 80.32442092895508, 258.7390327453613, 352.1267795562744, 1.4473958313465118, 249.16021347045898, 209.7802734375, -3.172488212585449, 323.0580139160156, 305.9751319885254, 78.83749008178711, 255.50432205200195, 199.9367094039917, -5.425000786781311, 269.9504852294922, 192.30402946472168, -5.672556161880493, 282.92367935180664, 189.91275787353516, -3.1538742780685425, 300.7317352294922, 195.17955780029297, 10.023578405380249, 270.7756233215332, 231.0417938232422, 3.5750579833984375, 256.40655517578125, 287.04219818115234, -46.028428077697754]\n",
      "training set:\n",
      "700\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "labels = {0: \"no_yawn\", 1: \"yawn\"}\n",
    "\n",
    "Y_train = [] #1d array, (big_number, 1)\n",
    "X_train = [] #2d array, (big_number, 42)\n",
    "def data_append_train(data, X_train = X_train, Y_train = Y_train):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/train\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_train.append([float(num) for num in row])\n",
    "                Y_train.append(label)\n",
    "    \n",
    "    print(X_train[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_train))\n",
    "    print(len(Y_train))    \n",
    "\n",
    "Y_test = []\n",
    "X_test = []\n",
    "def data_append_test(data, X_test = X_test, Y_test = Y_test):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/test\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_test.append([float(num) for num in row])\n",
    "                Y_test.append(label)\n",
    "    \n",
    "    print(X_test[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_test))\n",
    "    print(len(Y_test))    \n",
    "#i'll only use the train test.\n",
    "data = \"data\"\n",
    "data_append_train(data)\n",
    "data_append_test(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since I already have the training dataset split, I just need to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[353.44478607, 276.75739288, -52.59527206, ..., 366.1561203 ,\n",
       "         275.91233253, -45.3466177 ],\n",
       "        [337.43492126, 301.17736816, -55.10536671, ..., 351.42662048,\n",
       "         297.77366638, -45.172925  ],\n",
       "        [317.25000381, 314.96389389, -41.72193527, ..., 329.83459473,\n",
       "         312.71109581, -37.76688814],\n",
       "        ...,\n",
       "        [381.67285919, 291.85976028, -44.33598995, ..., 390.51780701,\n",
       "         288.99355888, -31.44557953],\n",
       "        [395.69858551, 326.77339554, -33.11842203, ..., 401.57093048,\n",
       "         327.70305634, -23.92225981],\n",
       "        [344.98332977, 302.50336647, -48.38704586, ..., 357.84194946,\n",
       "         299.94438171, -40.4253149 ]]),\n",
       " array([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_df_train = pd.DataFrame(X_train)\n",
    "# Y_df_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# X_df_test = pd.DataFrame(X_test)\n",
    "# Y_df_test = pd.DataFrame(Y_test)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "# \n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# the random_state is so that both shuffle perform the same.\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_test,Y_test = shuffle(X_test, Y_test, random_state=0)\n",
    "# X_train.shape, Y_train.shape\n",
    "X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 \n",
    "# model_save_path = \"model/\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model/yawn_model/model.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    #everytime the accuracy gets better, it saves\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (990, 174)\n",
      "Shape of X_test: (700, 174)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(units=32, activation='relu', input_shape=(174,)),\n",
    "    Dense(units=64, activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.05)),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    # Dropout(0.2),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=16, activation='relu', \n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    # Dropout(0.2),\n",
    "    Dense(units=NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "30/31 [============================>.] - ETA: 0s - loss: 19.1909 - accuracy: 0.6062INFO:tensorflow:Assets written to: model/yawn_model\\model.01-0.54\\assets\n",
      "31/31 [==============================] - 7s 170ms/step - loss: 19.0029 - accuracy: 0.6131 - val_loss: 13.1956 - val_accuracy: 0.5443\n",
      "Epoch 2/150\n",
      "31/31 [==============================] - ETA: 0s - loss: 11.9540 - accuracy: 0.7030INFO:tensorflow:Assets written to: model/yawn_model\\model.02-0.70\\assets\n",
      "31/31 [==============================] - 5s 154ms/step - loss: 11.9540 - accuracy: 0.7030 - val_loss: 11.0032 - val_accuracy: 0.7029\n",
      "Epoch 3/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 9.8829 - accuracy: 0.7879 - val_loss: 9.3592 - val_accuracy: 0.5614\n",
      "Epoch 4/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 8.1583 - accuracy: 0.8404 - val_loss: 7.6778 - val_accuracy: 0.6443\n",
      "Epoch 5/150\n",
      "29/31 [===========================>..] - ETA: 0s - loss: 6.7207 - accuracy: 0.8912INFO:tensorflow:Assets written to: model/yawn_model\\model.05-0.86\\assets\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 6.6804 - accuracy: 0.8929 - val_loss: 6.1796 - val_accuracy: 0.8571\n",
      "Epoch 6/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 5.5732 - accuracy: 0.9111 - val_loss: 5.3030 - val_accuracy: 0.8400\n",
      "Epoch 7/150\n",
      "24/31 [======================>.......] - ETA: 0s - loss: 4.7386 - accuracy: 0.9818INFO:tensorflow:Assets written to: model/yawn_model\\model.07-0.95\\assets\n",
      "31/31 [==============================] - 5s 152ms/step - loss: 4.6742 - accuracy: 0.9727 - val_loss: 4.4637 - val_accuracy: 0.9514\n",
      "Epoch 8/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 4.1978 - accuracy: 0.9394 - val_loss: 4.2532 - val_accuracy: 0.8743\n",
      "Epoch 9/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 3.8087 - accuracy: 0.9960 - val_loss: 3.8942 - val_accuracy: 0.9057\n",
      "Epoch 10/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 3.6191 - accuracy: 0.9980 - val_loss: 3.7545 - val_accuracy: 0.9171\n",
      "Epoch 11/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 3.4695 - accuracy: 0.9990 - val_loss: 3.5966 - val_accuracy: 0.9286\n",
      "Epoch 12/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 3.3247 - accuracy: 1.0000 - val_loss: 3.5378 - val_accuracy: 0.9171\n",
      "Epoch 13/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 3.1956 - accuracy: 0.9980 - val_loss: 3.9521 - val_accuracy: 0.6857\n",
      "Epoch 14/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 4.7968 - accuracy: 0.6859 - val_loss: 3.7128 - val_accuracy: 0.6757\n",
      "Epoch 15/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 3.2275 - accuracy: 0.9313 - val_loss: 3.2168 - val_accuracy: 0.8871\n",
      "Epoch 16/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.9251 - accuracy: 0.9899 - val_loss: 2.9930 - val_accuracy: 0.9300\n",
      "Epoch 17/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 2.7632 - accuracy: 1.0000 - val_loss: 2.8593 - val_accuracy: 0.9343\n",
      "Epoch 18/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 2.6481 - accuracy: 1.0000 - val_loss: 2.8079 - val_accuracy: 0.9286\n",
      "Epoch 19/150\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 2.5383 - accuracy: 1.0000 - val_loss: 2.7188 - val_accuracy: 0.9286\n",
      "Epoch 20/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 2.4351 - accuracy: 1.0000 - val_loss: 2.5903 - val_accuracy: 0.9329\n",
      "Epoch 21/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 2.3437 - accuracy: 0.9980 - val_loss: 2.6207 - val_accuracy: 0.9157\n",
      "Epoch 22/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 2.2581 - accuracy: 0.9960 - val_loss: 2.3640 - val_accuracy: 0.9386\n",
      "Epoch 23/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 2.7452 - accuracy: 0.8222 - val_loss: 2.8499 - val_accuracy: 0.5014\n",
      "Epoch 24/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 2.7742 - accuracy: 0.6394 - val_loss: 2.7447 - val_accuracy: 0.5486\n",
      "Epoch 25/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.6469 - accuracy: 0.6768 - val_loss: 2.7950 - val_accuracy: 0.4971\n",
      "Epoch 26/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 2.5168 - accuracy: 0.6788 - val_loss: 3.2067 - val_accuracy: 0.4971\n",
      "Epoch 27/150\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 2.4525 - accuracy: 0.6879 - val_loss: 2.6640 - val_accuracy: 0.5014\n",
      "Epoch 28/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 2.3166 - accuracy: 0.7162 - val_loss: 2.5381 - val_accuracy: 0.5014\n",
      "Epoch 29/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 2.2065 - accuracy: 0.7121 - val_loss: 2.3802 - val_accuracy: 0.5114\n",
      "Epoch 30/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 2.1185 - accuracy: 0.7606 - val_loss: 2.4014 - val_accuracy: 0.5229\n",
      "Epoch 31/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 1.9297 - accuracy: 0.8444 - val_loss: 2.0387 - val_accuracy: 0.8114\n",
      "Epoch 32/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 1.6974 - accuracy: 0.9465 - val_loss: 2.1274 - val_accuracy: 0.7343\n",
      "Epoch 33/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 1.6746 - accuracy: 0.8939 - val_loss: 1.7240 - val_accuracy: 0.8800\n",
      "Epoch 34/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 1.5932 - accuracy: 0.9152 - val_loss: 1.7295 - val_accuracy: 0.8943\n",
      "Epoch 35/150\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 1.4096 - accuracy: 0.9768 - val_loss: 1.8215 - val_accuracy: 0.8857\n",
      "Epoch 36/150\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.2885 - accuracy: 0.9980 - val_loss: 2.0187 - val_accuracy: 0.8414\n",
      "Epoch 37/150\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.2525 - accuracy: 0.9939 - val_loss: 1.5761 - val_accuracy: 0.9171\n",
      "Epoch 38/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 1.1816 - accuracy: 0.9990 - val_loss: 1.6361 - val_accuracy: 0.9071\n",
      "Epoch 39/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 1.1349 - accuracy: 1.0000 - val_loss: 1.5250 - val_accuracy: 0.9114\n",
      "Epoch 40/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 1.0907 - accuracy: 0.9990 - val_loss: 1.6465 - val_accuracy: 0.8971\n",
      "Epoch 41/150\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 1.0555 - accuracy: 0.9990 - val_loss: 1.6182 - val_accuracy: 0.9000\n",
      "Epoch 42/150\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 1.0226 - accuracy: 0.9960 - val_loss: 1.3984 - val_accuracy: 0.9243\n",
      "Epoch 43/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.9880 - accuracy: 0.9990 - val_loss: 1.2765 - val_accuracy: 0.9100\n",
      "Epoch 44/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.9506 - accuracy: 0.9980 - val_loss: 1.3074 - val_accuracy: 0.9243\n",
      "Epoch 45/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.9078 - accuracy: 1.0000 - val_loss: 1.4192 - val_accuracy: 0.9057\n",
      "Epoch 46/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.8753 - accuracy: 1.0000 - val_loss: 1.4034 - val_accuracy: 0.9043\n",
      "Epoch 47/150\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.8506 - accuracy: 0.9980 - val_loss: 1.3364 - val_accuracy: 0.9086\n",
      "Epoch 48/150\n",
      "31/31 [==============================] - 1s 20ms/step - loss: 0.8183 - accuracy: 0.9990 - val_loss: 1.2095 - val_accuracy: 0.9200\n",
      "Epoch 49/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.7901 - accuracy: 0.9990 - val_loss: 1.1877 - val_accuracy: 0.9257\n",
      "Epoch 50/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.7591 - accuracy: 1.0000 - val_loss: 1.2452 - val_accuracy: 0.9086\n",
      "Epoch 51/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.7336 - accuracy: 0.9990 - val_loss: 1.1130 - val_accuracy: 0.9271\n",
      "Epoch 52/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.7114 - accuracy: 0.9990 - val_loss: 1.0204 - val_accuracy: 0.9343\n",
      "Epoch 53/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.6864 - accuracy: 0.9980 - val_loss: 1.1405 - val_accuracy: 0.9214\n",
      "Epoch 54/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.6622 - accuracy: 0.9990 - val_loss: 1.0575 - val_accuracy: 0.9229\n",
      "Epoch 55/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.6329 - accuracy: 1.0000 - val_loss: 1.1727 - val_accuracy: 0.9143\n",
      "Epoch 56/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.6113 - accuracy: 1.0000 - val_loss: 0.9263 - val_accuracy: 0.9357\n",
      "Epoch 57/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.5893 - accuracy: 1.0000 - val_loss: 1.0143 - val_accuracy: 0.9143\n",
      "Epoch 58/150\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.5707 - accuracy: 1.0000 - val_loss: 0.9539 - val_accuracy: 0.9243\n",
      "Epoch 59/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.5519 - accuracy: 1.0000 - val_loss: 0.8949 - val_accuracy: 0.9300\n",
      "Epoch 60/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.5323 - accuracy: 1.0000 - val_loss: 1.1067 - val_accuracy: 0.9071\n",
      "Epoch 61/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.7688 - accuracy: 0.9303 - val_loss: 1.5616 - val_accuracy: 0.6543\n",
      "Epoch 62/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.7884 - accuracy: 0.9283 - val_loss: 0.8747 - val_accuracy: 0.8743\n",
      "Epoch 63/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.6780 - accuracy: 0.9444 - val_loss: 0.7776 - val_accuracy: 0.8986\n",
      "Epoch 64/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.5276 - accuracy: 0.9869 - val_loss: 0.8163 - val_accuracy: 0.9057\n",
      "Epoch 65/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.4637 - accuracy: 1.0000 - val_loss: 0.8660 - val_accuracy: 0.9071\n",
      "Epoch 66/150\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 0.4445 - accuracy: 1.0000 - val_loss: 0.7528 - val_accuracy: 0.9329\n",
      "Epoch 67/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.4493 - accuracy: 0.9939 - val_loss: 0.8940 - val_accuracy: 0.9100\n",
      "Epoch 68/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.5651 - accuracy: 0.9556 - val_loss: 0.7764 - val_accuracy: 0.9086\n",
      "Epoch 69/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.4242 - accuracy: 0.9960 - val_loss: 0.7289 - val_accuracy: 0.9243\n",
      "Epoch 70/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.3973 - accuracy: 0.9990 - val_loss: 0.7975 - val_accuracy: 0.9171\n",
      "Epoch 71/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.3799 - accuracy: 1.0000 - val_loss: 0.8555 - val_accuracy: 0.9086\n",
      "Epoch 72/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.3682 - accuracy: 1.0000 - val_loss: 0.7740 - val_accuracy: 0.9200\n",
      "Epoch 73/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.3575 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.9229\n",
      "Epoch 74/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.3464 - accuracy: 1.0000 - val_loss: 0.8267 - val_accuracy: 0.9171\n",
      "Epoch 75/150\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.3359 - accuracy: 1.0000 - val_loss: 0.8042 - val_accuracy: 0.9186\n",
      "Epoch 76/150\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.3260 - accuracy: 1.0000 - val_loss: 0.7850 - val_accuracy: 0.9186\n",
      "Epoch 77/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.8474 - val_accuracy: 0.9129\n",
      "Epoch 78/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.3080 - accuracy: 1.0000 - val_loss: 0.7991 - val_accuracy: 0.9171\n",
      "Epoch 79/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.2994 - accuracy: 1.0000 - val_loss: 0.7430 - val_accuracy: 0.9186\n",
      "Epoch 80/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.3121 - accuracy: 0.9929 - val_loss: 0.9803 - val_accuracy: 0.8943\n",
      "Epoch 81/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.3456 - accuracy: 0.9859 - val_loss: 0.6832 - val_accuracy: 0.9000\n",
      "Epoch 82/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.3005 - accuracy: 0.9949 - val_loss: 0.5265 - val_accuracy: 0.9371\n",
      "Epoch 83/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.3526 - accuracy: 0.9727 - val_loss: 5.4223 - val_accuracy: 0.4886\n",
      "Epoch 84/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 1.1449 - accuracy: 0.6747 - val_loss: 0.6511 - val_accuracy: 0.8700\n",
      "Epoch 85/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.4034 - accuracy: 0.9788 - val_loss: 0.6647 - val_accuracy: 0.9171\n",
      "Epoch 86/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.3005 - accuracy: 0.9919 - val_loss: 0.7869 - val_accuracy: 0.8971\n",
      "Epoch 87/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.2722 - accuracy: 0.9970 - val_loss: 0.4520 - val_accuracy: 0.9414\n",
      "Epoch 88/150\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.2458 - accuracy: 1.0000 - val_loss: 0.5458 - val_accuracy: 0.9286\n",
      "Epoch 89/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.2330 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 0.9186\n",
      "Epoch 90/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.2236 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.9371\n",
      "Epoch 91/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.2164 - accuracy: 1.0000 - val_loss: 0.5678 - val_accuracy: 0.9243\n",
      "Epoch 92/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.2079 - accuracy: 1.0000 - val_loss: 0.5690 - val_accuracy: 0.9243\n",
      "Epoch 93/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.2011 - accuracy: 1.0000 - val_loss: 0.5557 - val_accuracy: 0.9300\n",
      "Epoch 94/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1946 - accuracy: 1.0000 - val_loss: 0.5377 - val_accuracy: 0.9329\n",
      "Epoch 95/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.1883 - accuracy: 1.0000 - val_loss: 0.5348 - val_accuracy: 0.9329\n",
      "Epoch 96/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1820 - accuracy: 1.0000 - val_loss: 0.4994 - val_accuracy: 0.9357\n",
      "Epoch 97/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.1767 - accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.9314\n",
      "Epoch 98/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1730 - accuracy: 1.0000 - val_loss: 0.5392 - val_accuracy: 0.9329\n",
      "Epoch 99/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.1687 - accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.9414\n",
      "Epoch 100/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.2177 - accuracy: 0.9838 - val_loss: 0.9510 - val_accuracy: 0.8471\n",
      "Epoch 101/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.2324 - accuracy: 0.9859 - val_loss: 0.4548 - val_accuracy: 0.9214\n",
      "Epoch 102/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.1782 - accuracy: 0.9990 - val_loss: 0.4314 - val_accuracy: 0.9386\n",
      "Epoch 103/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1635 - accuracy: 1.0000 - val_loss: 0.4653 - val_accuracy: 0.9343\n",
      "Epoch 104/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1559 - accuracy: 1.0000 - val_loss: 0.4322 - val_accuracy: 0.9414\n",
      "Epoch 105/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.1503 - accuracy: 1.0000 - val_loss: 0.5131 - val_accuracy: 0.9314\n",
      "Epoch 106/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.1456 - accuracy: 1.0000 - val_loss: 0.5251 - val_accuracy: 0.9329\n",
      "Epoch 107/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.1413 - accuracy: 1.0000 - val_loss: 0.5310 - val_accuracy: 0.9329\n",
      "Epoch 108/150\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.5293 - val_accuracy: 0.9343\n",
      "Epoch 109/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1337 - accuracy: 1.0000 - val_loss: 0.5413 - val_accuracy: 0.9329\n",
      "Epoch 110/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1303 - accuracy: 1.0000 - val_loss: 0.5294 - val_accuracy: 0.9343\n",
      "Epoch 111/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 0.5566 - val_accuracy: 0.9329\n",
      "Epoch 112/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1242 - accuracy: 1.0000 - val_loss: 0.5526 - val_accuracy: 0.9329\n",
      "Epoch 113/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 0.5214 - val_accuracy: 0.9371\n",
      "Epoch 114/150\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.1184 - accuracy: 1.0000 - val_loss: 0.5420 - val_accuracy: 0.9343\n",
      "Epoch 115/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.1156 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.9343\n",
      "Epoch 116/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1129 - accuracy: 1.0000 - val_loss: 0.5392 - val_accuracy: 0.9343\n",
      "Epoch 117/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1101 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.9329\n",
      "Epoch 118/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 0.5448 - val_accuracy: 0.9343\n",
      "Epoch 119/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1050 - accuracy: 1.0000 - val_loss: 0.5339 - val_accuracy: 0.9357\n",
      "Epoch 120/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1026 - accuracy: 1.0000 - val_loss: 0.5497 - val_accuracy: 0.9329\n",
      "Epoch 121/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.1002 - accuracy: 1.0000 - val_loss: 0.5380 - val_accuracy: 0.9357\n",
      "Epoch 122/150\n",
      "31/31 [==============================] - 1s 21ms/step - loss: 0.0979 - accuracy: 1.0000 - val_loss: 0.5455 - val_accuracy: 0.9343\n",
      "Epoch 123/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.5482 - val_accuracy: 0.9343\n",
      "Epoch 124/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0932 - accuracy: 1.0000 - val_loss: 0.5411 - val_accuracy: 0.9343\n",
      "Epoch 125/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0909 - accuracy: 1.0000 - val_loss: 0.5552 - val_accuracy: 0.9329\n",
      "Epoch 126/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0886 - accuracy: 1.0000 - val_loss: 0.5636 - val_accuracy: 0.9329\n",
      "Epoch 127/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.5437 - val_accuracy: 0.9329\n",
      "Epoch 128/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0844 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.9386\n",
      "Epoch 129/150\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.0824 - accuracy: 1.0000 - val_loss: 0.5396 - val_accuracy: 0.9343\n",
      "Epoch 130/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.0805 - accuracy: 1.0000 - val_loss: 0.5360 - val_accuracy: 0.9343\n",
      "Epoch 131/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0786 - accuracy: 1.0000 - val_loss: 0.5661 - val_accuracy: 0.9343\n",
      "Epoch 132/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0767 - accuracy: 1.0000 - val_loss: 0.5488 - val_accuracy: 0.9357\n",
      "Epoch 133/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 0.5462 - val_accuracy: 0.9343\n",
      "Epoch 134/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 0.5393 - val_accuracy: 0.9357\n",
      "Epoch 135/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 0.5040 - val_accuracy: 0.9400\n",
      "Epoch 136/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0694 - accuracy: 1.0000 - val_loss: 0.5421 - val_accuracy: 0.9343\n",
      "Epoch 137/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0674 - accuracy: 1.0000 - val_loss: 0.5447 - val_accuracy: 0.9343\n",
      "Epoch 138/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.0658 - accuracy: 1.0000 - val_loss: 0.4776 - val_accuracy: 0.9414\n",
      "Epoch 139/150\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.5566 - val_accuracy: 0.9343\n",
      "Epoch 140/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 0.5613 - val_accuracy: 0.9357\n",
      "Epoch 141/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.5225 - val_accuracy: 0.9386\n",
      "Epoch 142/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 0.5401 - val_accuracy: 0.9357\n",
      "Epoch 143/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 0.5567 - val_accuracy: 0.9357\n",
      "Epoch 144/150\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0572 - accuracy: 1.0000 - val_loss: 0.5003 - val_accuracy: 0.9386\n",
      "Epoch 145/150\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.0645 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.9443\n",
      "Epoch 146/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.3432 - accuracy: 0.9717 - val_loss: 12.1971 - val_accuracy: 0.4829\n",
      "Epoch 147/150\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 2.4364 - accuracy: 0.6677 - val_loss: 1.1074 - val_accuracy: 0.4829\n",
      "Epoch 148/150\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.6884 - accuracy: 0.6869 - val_loss: 0.8510 - val_accuracy: 0.5343\n",
      "Epoch 149/150\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.7412 - accuracy: 0.7101 - val_loss: 0.8997 - val_accuracy: 0.5100\n",
      "Epoch 150/150\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.7145 - accuracy: 0.7152 - val_loss: 0.8630 - val_accuracy: 0.5314\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(X_train,Y_train, epochs=150, batch_size=32, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
