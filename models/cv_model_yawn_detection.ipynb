{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### work in this repository was inspired by \n",
    "##### https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe/blob/main/README_EN.md\n",
    "##### https://www.youtube.com/watch?v=a99p_fAr6e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# this is just to make sure that the results are reproducible to anyone that runs the code lol.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data formatting\n",
    "https://devqa.io/python-convert-csv-file-to-list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_yawn\n",
      "['data/train/no_yawn\\\\eye2.csv']\n",
      "1\n",
      "[222.25069046020508, 233.9298677444458, -36.499876976013184, 222.02556610107422, 225.97487926483154, -38.721935749053955, 221.22488021850586, 215.78165531158447, -35.770583152770996, 218.77561569213867, 191.43005847930908, -16.770299673080444, 217.38073348999023, 173.00106525421143, -11.76937460899353, 222.51628875732422, 268.6786937713623, -14.411613941192627, 216.42608642578125, 234.080171585083, -36.190924644470215, 185.33504486083984, 271.6083526611328, -1.4509478211402893, 148.03768157958984, 269.67607498168945, 57.80587196350098, 204.4702911376953, 237.68178462982178, -17.349413633346558, 198.54713439941406, 237.12430000305176, -16.496912240982056, 199.32052612304688, 272.8155040740967, -7.263230085372925, 215.9865951538086, 268.97552490234375, -14.164596796035767, 205.2019500732422, 270.32618522644043, -9.641178846359253, 204.17892456054688, 271.6710376739502, -10.759450197219849, 137.58285522460938, 232.24735736846924, 68.74664783477783, 150.85023880004883, 183.81049633026123, 11.467598676681519, 141.63026809692383, 250.67770957946777, 65.15983581542969, 166.05743408203125, 294.6782970428467, 33.61693859100342, 208.91143798828125, 315.54957389831543, 2.7877837419509888, 187.33238220214844, 308.63101959228516, 15.654668807983398, 177.6702117919922, 303.2472610473633, 23.439693450927734, 223.13304901123047, 315.6375503540039, 1.6228324174880981, 217.88822174072266, 182.2397232055664, -11.5880286693573, 156.54104232788086, 284.3751525878906, 46.22727870941162, 197.5132179260254, 312.9440116882324, 7.6930660009384155, 210.11966705322266, 269.48018074035645, -12.233202457427979, 196.97881698608398, 185.30595302581787, 3.2199054956436157, 220.38274765014648, 207.41233348846436, -29.202959537506104, 219.5924949645996, 199.6651840209961, -22.738122940063477, 183.33484649658203, 172.92523384094238, -3.236812651157379, 164.2913055419922, 173.99989128112793, -1.0416682809591293, 135.30485153198242, 214.94061470031738, 68.83233547210693, 283.5308837890625, 263.1324005126953, 67.43216514587402, 242.33766555786133, 234.77120876312256, -13.771928548812866, 243.8545036315918, 269.6678352355957, -3.868732452392578, 228.5576057434082, 268.0945014953613, -13.06577444076538, 239.3994140625, 269.1660976409912, -8.10501217842102, 287.9739761352539, 225.7981538772583, 79.22654151916504, 286.7435836791992, 244.12199020385742, 75.33098220825195, 271.6168212890625, 288.7023639678955, 40.80027103424072, 289.81285095214844, 224.17236328125, 53.998708724975586, 279.9404525756836, 270.4409694671631, 37.99152612686157, 236.46032333374023, 313.57921600341797, 4.625828564167023, 254.69104766845703, 304.1199016571045, 20.280635356903076, 262.5748062133789, 297.9371738433838, 29.266488552093506, 278.2985496520996, 278.02425384521484, 54.80507850646973, 246.34572982788086, 309.4881820678711, 11.0820472240448, 288.36376190185547, 241.038236618042, 52.26985454559326, 233.76243591308594, 267.7481746673584, -10.238288640975952, 232.02491760253906, 183.76808166503906, 3.0430150032043457, 285.68782806396484, 256.4798927307129, 48.455843925476074, 236.50001525878906, 176.7474603652954, 1.3517095148563385, 246.4055633544922, 171.32662296295166, 0.7433095574378967, 255.4616928100586, 169.84116554260254, 1.8107062578201294, 268.41808319091797, 174.36970710754395, 10.03487229347229, 246.17143630981445, 199.75706577301025, 4.8393332958221436, 232.23983764648438, 232.31311798095703, -31.867663860321045]\n",
      "training set:\n",
      "1861\n",
      "1861\n",
      "yawn\n",
      "['data/train/yawn\\\\eye2.csv']\n",
      "1\n",
      "[376.99893951416016, 192.11628913879395, -54.78672504425049, 377.340087890625, 181.18017196655273, -57.82217502593994, 377.3092269897461, 167.40707874298096, -53.513450622558594, 376.83589935302734, 133.7724494934082, -25.72993040084839, 376.91883087158203, 104.51613664627075, -18.439810276031494, 376.66046142578125, 286.80779457092285, -12.889653444290161, 368.9414978027344, 191.7659568786621, -54.11661624908447, 322.9764938354492, 253.01874160766602, 3.1562012434005737, 273.4682846069336, 238.73733043670654, 83.20021629333496, 352.19364166259766, 197.13595390319824, -26.858127117156982, 343.62560272216797, 196.80121421813965, -25.469164848327637, 340.5963897705078, 273.9251518249512, -3.620624840259552, 365.8881378173828, 285.3984260559082, -12.20032811164856, 350.55797576904297, 277.7328872680664, -6.408767104148865, 348.2115936279297, 279.0567398071289, -6.973939538002014, 259.4968795776367, 184.07793045043945, 97.10886001586914, 281.56322479248047, 116.4383053779602, 16.69407367706299, 264.1919708251953, 210.00802516937256, 92.69094467163086, 296.3164520263672, 279.36304092407227, 53.74890327453613, 349.64611053466797, 326.93796157836914, 21.038193702697754, 320.65704345703125, 306.81873321533203, 35.04640579223633, 309.34173583984375, 295.02196311950684, 42.97926425933838, 371.1737060546875, 330.41507720947266, 19.548345804214478, 376.70032501220703, 119.50529336929321, -18.36620807647705, 285.05767822265625, 262.322473526001, 68.15444946289062, 333.56239318847656, 318.1742477416992, 26.495506763458252, 357.3499298095703, 282.0929718017578, -9.72577691078186, 346.28997802734375, 121.4024305343628, 2.81294047832489, 377.11009979248047, 156.11207485198975, -43.92560005187988, 376.9465637207031, 145.51225662231445, -34.37169790267944, 328.7717819213867, 101.88765048980713, -5.128573179244995, 302.09203720092773, 101.81417942047119, -1.075451448559761, 257.4232292175293, 160.12842178344727, 96.85754776000977, 462.8330612182617, 242.1452808380127, 89.99584197998047, 406.3090133666992, 197.72581100463867, -23.7581729888916, 405.5536651611328, 275.18277168273926, -1.092885360121727, 386.116943359375, 285.51652908325195, -11.355289220809937, 399.9900817871094, 279.8606300354004, -4.934085011482239, 477.5456237792969, 189.0730619430542, 104.52199935913086, 472.60459899902344, 214.35184478759766, 99.82879638671875, 440.80482482910156, 281.21758460998535, 58.77915382385254, 479.79183197021484, 187.28028774261475, 68.26817512512207, 453.13892364501953, 254.6070671081543, 51.488399505615234, 391.93653106689453, 327.20787048339844, 22.377078533172607, 418.73058319091797, 307.6475715637207, 38.3264684677124, 429.06803131103516, 296.27577781677246, 47.06254005432129, 451.373291015625, 264.8269844055176, 74.29802417755127, 406.8954086303711, 318.6453437805176, 28.8704514503479, 474.4263458251953, 211.191987991333, 66.67132377624512, 392.8384780883789, 282.48647689819336, -8.222383856773376, 395.20816802978516, 120.87512969970703, 0.5554251372814178, 465.83091735839844, 233.6109495162964, 62.91271686553955, 404.0486145019531, 110.99757671356201, -1.8110910058021545, 421.01844787597656, 105.21883964538574, -2.8437265753746033, 435.32691955566406, 104.81791019439697, -1.7231950163841248, 453.27880859375, 113.26255559921265, 9.009913206100464, 414.09446716308594, 147.53179550170898, 2.260396182537079, 391.3446044921875, 190.80901622772217, -48.78556728363037]\n",
      "training set:\n",
      "3308\n",
      "3308\n",
      "no_yawn\n",
      "['data/test/no_yawn\\\\eye.csv']\n",
      "1\n",
      "[482.1818161010742, 261.08842849731445, -51.26882553100586, 482.83275604248047, 249.94465827941895, -54.807071685791016, 483.38367462158203, 235.01448154449463, -51.23660087585449, 484.45674896240234, 198.73334884643555, -25.948598384857178, 485.3153991699219, 171.48001670837402, -20.120704174041748, 480.4308319091797, 307.540225982666, -17.005282640457153, 474.0159225463867, 260.39076805114746, -50.500359535217285, 427.48058319091797, 306.4533805847168, 4.116925001144409, 380.0724411010742, 297.01257705688477, 89.64959144592285, 457.6869583129883, 262.29397773742676, -22.974705696105957, 449.66480255126953, 260.500431060791, -21.37593984603882, 445.85205078125, 308.9824962615967, -4.639378786087036, 470.9140396118164, 306.6961669921875, -16.00742816925049, 454.92103576660156, 306.23897552490234, -8.741504549980164, 453.2466506958008, 308.2125663757324, -10.058101415634155, 371.52099609375, 242.69737243652344, 102.5670337677002, 392.9051208496094, 177.0439910888672, 17.373478412628174, 374.07745361328125, 269.27770614624023, 98.79946708679199, 400.6540298461914, 335.49951553344727, 56.20986461639404, 454.6930694580078, 373.35153579711914, 12.450660467147827, 426.82857513427734, 359.1982841491699, 31.031458377838135, 414.7999572753906, 349.6841812133789, 41.912851333618164, 474.4437026977539, 375.8544731140137, 9.950668811798096, 484.8219680786133, 184.99613285064697, -19.267162084579468, 389.527587890625, 319.2287063598633, 73.86888980865479, 439.65599060058594, 367.44375228881836, 19.788854122161865, 462.2883987426758, 306.1010456085205, -12.81043529510498, 458.6590576171875, 185.2136993408203, 3.0586177110671997, 483.7603759765625, 222.54953384399414, -42.45612144470215, 484.0934371948242, 211.00167274475098, -33.8797402381897, 439.46533203125, 166.8350887298584, -5.981004238128662, 412.0964813232422, 166.44115447998047, -1.4864523708820343, 371.01165771484375, 218.06425094604492, 101.25782012939453, 574.9592208862305, 304.91031646728516, 92.35588073730469, 512.6219177246094, 263.2585144042969, -20.6081223487854, 511.0020065307617, 312.1394348144531, -3.5308045148849487, 489.2620086669922, 307.6258850097656, -15.687342882156372, 504.60121154785156, 310.67235946655273, -9.185864925384521, 588.2770538330078, 251.7798614501953, 105.49103736877441, 583.4561157226562, 277.972412109375, 101.6860580444336, 551.0050201416016, 341.15123748779297, 58.265438079833984, 588.7511825561523, 250.56607246398926, 70.08880138397217, 565.5774307250977, 315.9694290161133, 52.42556571960449, 494.1598129272461, 374.8101997375488, 13.060853481292725, 522.761344909668, 362.5660514831543, 32.37419366836548, 535.6909561157227, 354.0429496765137, 43.64744186401367, 563.6018753051758, 325.962610244751, 76.27861976623535, 509.3918228149414, 369.91418838500977, 20.774354934692383, 583.8970947265625, 274.4667720794678, 69.1646957397461, 496.8375015258789, 307.76206970214844, -12.174301147460938, 506.9560241699219, 185.57650566101074, 0.08226437494158745, 577.1153259277344, 296.30693435668945, 65.35274505615234, 515.2457046508789, 176.67780876159668, -3.0625781416893005, 531.3644027709961, 170.76534748077393, -4.68711256980896, 545.3300094604492, 170.00800609588623, -3.789830207824707, 564.2354583740234, 177.68553256988525, 6.796239614486694, 524.1601181030273, 210.26957988739014, 2.6640883088111877, 497.05284118652344, 259.6634101867676, -45.5497407913208]\n",
      "training set:\n",
      "338\n",
      "338\n",
      "yawn\n",
      "['data/test/yawn\\\\eye.csv']\n",
      "1\n",
      "[242.79800415039062, 290.7513999938965, -53.21986675262451, 242.30403900146484, 279.64213371276855, -57.307701110839844, 240.27631759643555, 264.4527339935303, -54.551873207092285, 233.8381004333496, 226.6416835784912, -31.77013874053955, 229.99088287353516, 197.7088737487793, -27.04880714416504, 243.59111785888672, 360.2245903015137, -6.451576352119446, 234.3002700805664, 291.1380672454834, -53.337745666503906, 183.8692855834961, 339.65749740600586, 6.001219153404236, 128.28760147094727, 330.386438369751, 76.538405418396, 214.0602684020996, 293.4077453613281, -27.530362606048584, 204.93751525878906, 291.663293838501, -26.98728084564209, 202.58432388305664, 352.9490089416504, 2.031026929616928, 231.9432258605957, 360.8141899108887, -6.931779980659485, 213.34226608276367, 355.68400382995605, -1.4682179689407349, 211.46602630615234, 357.0182991027832, -2.056663930416107, 109.02485847473145, 275.577449798584, 82.51452445983887, 129.2885398864746, 211.49714469909668, -1.2447410821914673, 116.42220497131348, 301.9485855102539, 81.74736976623535, 158.3470344543457, 371.00830078125, 53.48684310913086, 222.86188125610352, 412.0220947265625, 27.341148853302002, 190.10719299316406, 396.36380195617676, 38.33338975906372, 175.7695770263672, 385.8489418029785, 44.88492488861084, 243.84933471679688, 412.46386528015137, 27.932004928588867, 231.4075469970703, 211.69238090515137, -25.773210525512695, 143.16740036010742, 353.86834144592285, 64.9931812286377, 205.4764175415039, 405.88937759399414, 31.04893207550049, 221.80278778076172, 359.132022857666, -4.844290614128113, 201.27607345581055, 213.6429262161255, -6.983250379562378, 238.04996490478516, 251.67054176330566, -46.56522274017334, 235.99685668945312, 239.7601318359375, -38.84204149246216, 178.12795639038086, 197.26407051086426, -18.059674501419067, 148.89599800109863, 199.50722694396973, -16.80443048477173, 104.62664604187012, 251.12537384033203, 78.84582042694092, 317.48363494873047, 313.0686664581299, 105.28581619262695, 269.6544647216797, 285.5567264556885, -18.74048113822937, 270.17162322998047, 342.6390266418457, 12.775959968566895, 252.7627182006836, 356.86503410339355, -3.3718979358673096, 264.99019622802734, 347.94633865356445, 6.358324289321899, 323.5367202758789, 258.1827449798584, 114.11618232727051, 322.21435546875, 284.2411136627197, 112.1103572845459, 302.8651809692383, 355.71081161499023, 75.15557289123535, 329.5993423461914, 258.4933376312256, 79.48703289031982, 314.74720001220703, 328.3490753173828, 68.1666374206543, 262.32418060302734, 406.93161964416504, 32.9796028137207, 284.3669891357422, 384.96336936950684, 52.40516662597656, 292.9559898376465, 372.3972702026367, 62.53363609313965, 310.58874130249023, 337.44489669799805, 90.76019287109375, 274.9391174316406, 397.07940101623535, 41.401591300964355, 327.8586196899414, 282.7152442932129, 80.32442092895508, 258.7390327453613, 352.1267795562744, 1.4473958313465118, 249.16021347045898, 209.7802734375, -3.172488212585449, 323.0580139160156, 305.9751319885254, 78.83749008178711, 255.50432205200195, 199.9367094039917, -5.425000786781311, 269.9504852294922, 192.30402946472168, -5.672556161880493, 282.92367935180664, 189.91275787353516, -3.1538742780685425, 300.7317352294922, 195.17955780029297, 10.023578405380249, 270.7756233215332, 231.0417938232422, 3.5750579833984375, 256.40655517578125, 287.04219818115234, -46.028428077697754]\n",
      "training set:\n",
      "700\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "labels = {0: \"no_yawn\", 1: \"yawn\"}\n",
    "\n",
    "Y_train = [] #1d array, (big_number, 1)\n",
    "X_train = [] #2d array, (big_number, 42)\n",
    "def data_append_train(data, X_train = X_train, Y_train = Y_train):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/train\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_train.append([float(num) for num in row])\n",
    "                Y_train.append(label)\n",
    "    \n",
    "    print(X_train[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_train))\n",
    "    print(len(Y_train))    \n",
    "\n",
    "Y_test = []\n",
    "X_test = []\n",
    "def data_append_test(data, X_test = X_test, Y_test = Y_test):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/test\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_test.append([float(num) for num in row])\n",
    "                Y_test.append(label)\n",
    "    \n",
    "    print(X_test[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_test))\n",
    "    print(len(Y_test))    \n",
    "#i'll only use the train test.\n",
    "data = \"data\"\n",
    "data_append_train(data)\n",
    "data_append_test(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since I already have the training dataset split, I just need to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[342.18013763, 260.15885353, -37.74030209, ..., 352.32494354,\n",
       "         260.89742661, -32.1662879 ],\n",
       "        [329.57881927, 205.42036057, -44.47785854, ..., 342.2070694 ,\n",
       "         203.77053738, -39.3476367 ],\n",
       "        [261.48199081, 217.17184067, -30.72162867, ..., 269.2628479 ,\n",
       "         215.71579456, -25.85042715],\n",
       "        ...,\n",
       "        [189.49829102, 237.83334732, -58.42456341, ..., 208.16854477,\n",
       "         233.92504692, -54.99604702],\n",
       "        [399.60113525, 239.40487862, -51.03535652, ..., 415.53142548,\n",
       "         238.31093788, -46.16221905],\n",
       "        [366.90582275, 233.97930622, -44.0205574 , ..., 379.80670929,\n",
       "         232.25850105, -38.63370895]]),\n",
       " array([0, 0, 1, ..., 0, 1, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_df_train = pd.DataFrame(X_train)\n",
    "# Y_df_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# X_df_test = pd.DataFrame(X_test)\n",
    "# Y_df_test = pd.DataFrame(Y_test)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "# \n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# the random_state is so that both shuffle perform the same.\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_test,Y_test = shuffle(X_test, Y_test, random_state=0)\n",
    "# X_train.shape, Y_train.shape\n",
    "X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 \n",
    "# model_save_path = \"model/\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model/yawn_model/model.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    #everytime the accuracy gets better, it saves\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (3308, 174)\n",
      "Shape of X_test: (700, 174)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(units=32, activation='relu', input_shape=(174,)),\n",
    "    Dense(units=64, activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.05)),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=16, activation='relu', \n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    # Dense(units=1, activation='sigmoid')\n",
    "    Dense(units = NUM_CLASSES, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "104/104 [==============================] - 3s 12ms/step - loss: 13.1171 - accuracy: 0.5320 - val_loss: 9.4841 - val_accuracy: 0.4829\n",
      "Epoch 2/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 7.4761 - accuracy: 0.5632 - val_loss: 5.9312 - val_accuracy: 0.4829\n",
      "Epoch 3/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 5.1162 - accuracy: 0.5626 - val_loss: 4.6566 - val_accuracy: 0.4829\n",
      "Epoch 4/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 4.2229 - accuracy: 0.5629 - val_loss: 3.9046 - val_accuracy: 0.4457\n",
      "Epoch 5/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 3.5264 - accuracy: 0.6291 - val_loss: 3.4313 - val_accuracy: 0.6357\n",
      "Epoch 6/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 3.0443 - accuracy: 0.6091 - val_loss: 2.8201 - val_accuracy: 0.6571\n",
      "Epoch 7/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 2.5871 - accuracy: 0.5735 - val_loss: 2.4536 - val_accuracy: 0.4829\n",
      "Epoch 8/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 2.2022 - accuracy: 0.5620 - val_loss: 2.1126 - val_accuracy: 0.4829\n",
      "Epoch 9/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 1.8768 - accuracy: 0.6294 - val_loss: 1.9090 - val_accuracy: 0.6571\n",
      "Epoch 10/150\n",
      "104/104 [==============================] - 1s 12ms/step - loss: 1.6165 - accuracy: 0.6956 - val_loss: 1.6268 - val_accuracy: 0.6857\n",
      "Epoch 11/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 1.4625 - accuracy: 0.6880 - val_loss: 1.4922 - val_accuracy: 0.4829\n",
      "Epoch 12/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 1.2898 - accuracy: 0.6696 - val_loss: 1.4407 - val_accuracy: 0.6843\n",
      "Epoch 13/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 1.1628 - accuracy: 0.7210 - val_loss: 1.3065 - val_accuracy: 0.6871\n",
      "Epoch 14/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.9887 - accuracy: 0.7597 - val_loss: 1.3947 - val_accuracy: 0.6886\n",
      "Epoch 15/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.8934 - accuracy: 0.7666 - val_loss: 1.3463 - val_accuracy: 0.7286\n",
      "Epoch 16/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.7914 - accuracy: 0.7805 - val_loss: 1.0096 - val_accuracy: 0.6900\n",
      "Epoch 17/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.7335 - accuracy: 0.7972 - val_loss: 1.0407 - val_accuracy: 0.7229\n",
      "Epoch 18/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.6177 - accuracy: 0.8389 - val_loss: 1.0353 - val_accuracy: 0.7529\n",
      "Epoch 19/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.6338 - accuracy: 0.8283 - val_loss: 0.9768 - val_accuracy: 0.7443\n",
      "Epoch 20/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.5301 - accuracy: 0.8606 - val_loss: 1.0811 - val_accuracy: 0.7457\n",
      "Epoch 21/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.4760 - accuracy: 0.8845 - val_loss: 0.8822 - val_accuracy: 0.7257\n",
      "Epoch 22/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.4574 - accuracy: 0.8821 - val_loss: 0.7547 - val_accuracy: 0.7443\n",
      "Epoch 23/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.4770 - accuracy: 0.8679 - val_loss: 0.7756 - val_accuracy: 0.7529\n",
      "Epoch 24/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.4419 - accuracy: 0.8752 - val_loss: 0.5502 - val_accuracy: 0.7943\n",
      "Epoch 25/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3784 - accuracy: 0.9036 - val_loss: 0.8729 - val_accuracy: 0.7700\n",
      "Epoch 26/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3419 - accuracy: 0.9120 - val_loss: 0.9821 - val_accuracy: 0.7314\n",
      "Epoch 27/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.4167 - accuracy: 0.8685 - val_loss: 0.5435 - val_accuracy: 0.8229\n",
      "Epoch 28/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3782 - accuracy: 0.8830 - val_loss: 0.5631 - val_accuracy: 0.7800\n",
      "Epoch 29/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3348 - accuracy: 0.9002 - val_loss: 0.4645 - val_accuracy: 0.8586\n",
      "Epoch 30/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3227 - accuracy: 0.9096 - val_loss: 0.7106 - val_accuracy: 0.7400\n",
      "Epoch 31/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3274 - accuracy: 0.8954 - val_loss: 0.6770 - val_accuracy: 0.7957\n",
      "Epoch 32/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.2959 - accuracy: 0.9105 - val_loss: 0.7530 - val_accuracy: 0.7529\n",
      "Epoch 33/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.2578 - accuracy: 0.9211 - val_loss: 1.5548 - val_accuracy: 0.6629\n",
      "Epoch 34/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.3246 - accuracy: 0.8921 - val_loss: 1.5152 - val_accuracy: 0.5700\n",
      "Epoch 35/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.2605 - accuracy: 0.9178 - val_loss: 1.0616 - val_accuracy: 0.7271\n",
      "Epoch 36/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3176 - accuracy: 0.8921 - val_loss: 0.7114 - val_accuracy: 0.7543\n",
      "Epoch 37/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2997 - accuracy: 0.8957 - val_loss: 0.8917 - val_accuracy: 0.7386\n",
      "Epoch 38/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2684 - accuracy: 0.9148 - val_loss: 0.7543 - val_accuracy: 0.8171\n",
      "Epoch 39/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2910 - accuracy: 0.9042 - val_loss: 0.5836 - val_accuracy: 0.7800\n",
      "Epoch 40/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2392 - accuracy: 0.9256 - val_loss: 0.5911 - val_accuracy: 0.8043\n",
      "Epoch 41/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2459 - accuracy: 0.9187 - val_loss: 1.1669 - val_accuracy: 0.7186\n",
      "Epoch 42/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.2473 - accuracy: 0.9202 - val_loss: 0.8386 - val_accuracy: 0.7571\n",
      "Epoch 43/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2546 - accuracy: 0.9163 - val_loss: 0.9863 - val_accuracy: 0.7400\n",
      "Epoch 44/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.3026 - accuracy: 0.8930 - val_loss: 0.6532 - val_accuracy: 0.7757\n",
      "Epoch 45/150\n",
      "100/104 [===========================>..] - ETA: 0s - loss: 0.2780 - accuracy: 0.8969INFO:tensorflow:Assets written to: model/yawn_model\\model.45-0.88\\assets\n",
      "104/104 [==============================] - 6s 57ms/step - loss: 0.2853 - accuracy: 0.8954 - val_loss: 0.4377 - val_accuracy: 0.8843\n",
      "Epoch 46/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2912 - accuracy: 0.9039 - val_loss: 0.7367 - val_accuracy: 0.7643\n",
      "Epoch 47/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2097 - accuracy: 0.9347 - val_loss: 0.8800 - val_accuracy: 0.7357\n",
      "Epoch 48/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.2227 - accuracy: 0.9268 - val_loss: 0.8908 - val_accuracy: 0.7357\n",
      "Epoch 49/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2277 - accuracy: 0.9235 - val_loss: 0.6999 - val_accuracy: 0.7229\n",
      "Epoch 50/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.3288 - accuracy: 0.8987 - val_loss: 0.6247 - val_accuracy: 0.7614\n",
      "Epoch 51/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2657 - accuracy: 0.9190 - val_loss: 0.5118 - val_accuracy: 0.8386\n",
      "Epoch 52/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2284 - accuracy: 0.9287 - val_loss: 1.0899 - val_accuracy: 0.7257\n",
      "Epoch 53/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2424 - accuracy: 0.9229 - val_loss: 0.8102 - val_accuracy: 0.7400\n",
      "Epoch 54/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2351 - accuracy: 0.9199 - val_loss: 1.2919 - val_accuracy: 0.7229\n",
      "Epoch 55/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2252 - accuracy: 0.9262 - val_loss: 0.8809 - val_accuracy: 0.7414\n",
      "Epoch 56/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2112 - accuracy: 0.9311 - val_loss: 1.4257 - val_accuracy: 0.7200\n",
      "Epoch 57/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2431 - accuracy: 0.9223 - val_loss: 0.7102 - val_accuracy: 0.7886\n",
      "Epoch 58/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2554 - accuracy: 0.9084 - val_loss: 2.2800 - val_accuracy: 0.6971\n",
      "Epoch 59/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.2476 - accuracy: 0.9151 - val_loss: 0.7406 - val_accuracy: 0.7329\n",
      "Epoch 60/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2019 - accuracy: 0.9341 - val_loss: 0.5998 - val_accuracy: 0.7943\n",
      "Epoch 61/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2048 - accuracy: 0.9338 - val_loss: 0.8303 - val_accuracy: 0.7329\n",
      "Epoch 62/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1987 - accuracy: 0.9374 - val_loss: 1.0220 - val_accuracy: 0.7114\n",
      "Epoch 63/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.3137 - accuracy: 0.8857 - val_loss: 0.8591 - val_accuracy: 0.7157\n",
      "Epoch 64/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2351 - accuracy: 0.9166 - val_loss: 0.9567 - val_accuracy: 0.7186\n",
      "Epoch 65/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2241 - accuracy: 0.9241 - val_loss: 1.5989 - val_accuracy: 0.6014\n",
      "Epoch 66/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2316 - accuracy: 0.9166 - val_loss: 0.7979 - val_accuracy: 0.7400\n",
      "Epoch 67/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2316 - accuracy: 0.9163 - val_loss: 0.5266 - val_accuracy: 0.8443\n",
      "Epoch 68/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2227 - accuracy: 0.9265 - val_loss: 0.6293 - val_accuracy: 0.7457\n",
      "Epoch 69/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2136 - accuracy: 0.9326 - val_loss: 0.7121 - val_accuracy: 0.7571\n",
      "Epoch 70/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2201 - accuracy: 0.9241 - val_loss: 0.7142 - val_accuracy: 0.7371\n",
      "Epoch 71/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2287 - accuracy: 0.9202 - val_loss: 1.4452 - val_accuracy: 0.7100\n",
      "Epoch 72/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2026 - accuracy: 0.9326 - val_loss: 0.5872 - val_accuracy: 0.7929\n",
      "Epoch 73/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2147 - accuracy: 0.9278 - val_loss: 0.6494 - val_accuracy: 0.7400\n",
      "Epoch 74/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1752 - accuracy: 0.9468 - val_loss: 0.7736 - val_accuracy: 0.7529\n",
      "Epoch 75/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2322 - accuracy: 0.9196 - val_loss: 1.5822 - val_accuracy: 0.7100\n",
      "Epoch 76/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1962 - accuracy: 0.9359 - val_loss: 0.7177 - val_accuracy: 0.7586\n",
      "Epoch 77/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2195 - accuracy: 0.9244 - val_loss: 0.8248 - val_accuracy: 0.7086\n",
      "Epoch 78/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1971 - accuracy: 0.9317 - val_loss: 1.0665 - val_accuracy: 0.7129\n",
      "Epoch 79/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1812 - accuracy: 0.9401 - val_loss: 0.7114 - val_accuracy: 0.7671\n",
      "Epoch 80/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2544 - accuracy: 0.9178 - val_loss: 1.0003 - val_accuracy: 0.7129\n",
      "Epoch 81/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1892 - accuracy: 0.9338 - val_loss: 0.5320 - val_accuracy: 0.7829\n",
      "Epoch 82/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.1779 - accuracy: 0.9462 - val_loss: 0.6554 - val_accuracy: 0.7600\n",
      "Epoch 83/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1996 - accuracy: 0.9287 - val_loss: 1.0662 - val_accuracy: 0.7243\n",
      "Epoch 84/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1967 - accuracy: 0.9308 - val_loss: 1.7684 - val_accuracy: 0.6914\n",
      "Epoch 85/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1919 - accuracy: 0.9323 - val_loss: 1.8161 - val_accuracy: 0.6914\n",
      "Epoch 86/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1732 - accuracy: 0.9462 - val_loss: 1.0571 - val_accuracy: 0.7471\n",
      "Epoch 87/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.1865 - accuracy: 0.9365 - val_loss: 0.9526 - val_accuracy: 0.7229\n",
      "Epoch 88/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2116 - accuracy: 0.9281 - val_loss: 0.7678 - val_accuracy: 0.7000\n",
      "Epoch 89/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2109 - accuracy: 0.9323 - val_loss: 0.8405 - val_accuracy: 0.7143\n",
      "Epoch 90/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1922 - accuracy: 0.9423 - val_loss: 1.2042 - val_accuracy: 0.7086\n",
      "Epoch 91/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1898 - accuracy: 0.9353 - val_loss: 1.3338 - val_accuracy: 0.6357\n",
      "Epoch 92/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2111 - accuracy: 0.9262 - val_loss: 0.9152 - val_accuracy: 0.6943\n",
      "Epoch 93/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.1828 - accuracy: 0.9350 - val_loss: 0.7438 - val_accuracy: 0.7300\n",
      "Epoch 94/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1958 - accuracy: 0.9344 - val_loss: 0.5137 - val_accuracy: 0.8071\n",
      "Epoch 95/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1763 - accuracy: 0.9450 - val_loss: 1.5185 - val_accuracy: 0.7043\n",
      "Epoch 96/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1820 - accuracy: 0.9365 - val_loss: 0.7280 - val_accuracy: 0.7343\n",
      "Epoch 97/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2373 - accuracy: 0.9066 - val_loss: 0.6289 - val_accuracy: 0.7143\n",
      "Epoch 98/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.2821 - accuracy: 0.9005 - val_loss: 1.0338 - val_accuracy: 0.6500\n",
      "Epoch 99/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1873 - accuracy: 0.9401 - val_loss: 0.9642 - val_accuracy: 0.6800\n",
      "Epoch 100/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1916 - accuracy: 0.9420 - val_loss: 0.7136 - val_accuracy: 0.7414\n",
      "Epoch 101/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1533 - accuracy: 0.9547 - val_loss: 1.9932 - val_accuracy: 0.6700\n",
      "Epoch 102/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1714 - accuracy: 0.9444 - val_loss: 2.2911 - val_accuracy: 0.6343\n",
      "Epoch 103/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1795 - accuracy: 0.9426 - val_loss: 0.7117 - val_accuracy: 0.7529\n",
      "Epoch 104/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1693 - accuracy: 0.9462 - val_loss: 0.7207 - val_accuracy: 0.7800\n",
      "Epoch 105/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1968 - accuracy: 0.9374 - val_loss: 1.2394 - val_accuracy: 0.6986\n",
      "Epoch 106/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1461 - accuracy: 0.9534 - val_loss: 0.7386 - val_accuracy: 0.7529\n",
      "Epoch 107/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1734 - accuracy: 0.9462 - val_loss: 0.9368 - val_accuracy: 0.7157\n",
      "Epoch 108/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1689 - accuracy: 0.9480 - val_loss: 1.3361 - val_accuracy: 0.6614\n",
      "Epoch 109/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.1834 - accuracy: 0.9341 - val_loss: 1.4232 - val_accuracy: 0.6557\n",
      "Epoch 110/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1799 - accuracy: 0.9525 - val_loss: 0.7332 - val_accuracy: 0.6943\n",
      "Epoch 111/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2064 - accuracy: 0.9423 - val_loss: 1.2400 - val_accuracy: 0.7114\n",
      "Epoch 112/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1436 - accuracy: 0.9562 - val_loss: 0.8789 - val_accuracy: 0.7814\n",
      "Epoch 113/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.2101 - accuracy: 0.9281 - val_loss: 1.5117 - val_accuracy: 0.6929\n",
      "Epoch 114/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1730 - accuracy: 0.9368 - val_loss: 0.9674 - val_accuracy: 0.7329\n",
      "Epoch 115/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.1765 - accuracy: 0.9411 - val_loss: 0.9105 - val_accuracy: 0.7171\n",
      "Epoch 116/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1663 - accuracy: 0.9453 - val_loss: 0.7757 - val_accuracy: 0.6600\n",
      "Epoch 117/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1645 - accuracy: 0.9435 - val_loss: 1.3487 - val_accuracy: 0.7014\n",
      "Epoch 118/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1767 - accuracy: 0.9407 - val_loss: 1.0991 - val_accuracy: 0.7043\n",
      "Epoch 119/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1743 - accuracy: 0.9392 - val_loss: 1.2904 - val_accuracy: 0.6971\n",
      "Epoch 120/150\n",
      "104/104 [==============================] - 1s 7ms/step - loss: 0.1968 - accuracy: 0.9293 - val_loss: 1.4204 - val_accuracy: 0.6743\n",
      "Epoch 121/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2047 - accuracy: 0.9268 - val_loss: 0.7745 - val_accuracy: 0.7157\n",
      "Epoch 122/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.2102 - accuracy: 0.9281 - val_loss: 0.6693 - val_accuracy: 0.7643\n",
      "Epoch 123/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2101 - accuracy: 0.9329 - val_loss: 1.2089 - val_accuracy: 0.6743\n",
      "Epoch 124/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1643 - accuracy: 0.9465 - val_loss: 1.0465 - val_accuracy: 0.7000\n",
      "Epoch 125/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1518 - accuracy: 0.9537 - val_loss: 1.5636 - val_accuracy: 0.6929\n",
      "Epoch 126/150\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1767 - accuracy: 0.9477 - val_loss: 0.8413 - val_accuracy: 0.7086\n",
      "Epoch 127/150\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.1822 - accuracy: 0.9426 - val_loss: 0.4541 - val_accuracy: 0.8100\n",
      "Epoch 128/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.2347 - accuracy: 0.9208 - val_loss: 1.1527 - val_accuracy: 0.7000\n",
      "Epoch 129/150\n",
      "104/104 [==============================] - 1s 12ms/step - loss: 0.1850 - accuracy: 0.9386 - val_loss: 1.2372 - val_accuracy: 0.6786\n",
      "Epoch 130/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1552 - accuracy: 0.9486 - val_loss: 0.8528 - val_accuracy: 0.7043\n",
      "Epoch 131/150\n",
      "104/104 [==============================] - 1s 12ms/step - loss: 0.1860 - accuracy: 0.9344 - val_loss: 1.7449 - val_accuracy: 0.6643\n",
      "Epoch 132/150\n",
      "104/104 [==============================] - 1s 12ms/step - loss: 0.1850 - accuracy: 0.9447 - val_loss: 1.0729 - val_accuracy: 0.6529\n",
      "Epoch 133/150\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.1330 - accuracy: 0.9646 - val_loss: 1.2027 - val_accuracy: 0.6757\n",
      "Epoch 134/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1943 - accuracy: 0.9278 - val_loss: 1.1607 - val_accuracy: 0.6900\n",
      "Epoch 135/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1445 - accuracy: 0.9519 - val_loss: 1.1411 - val_accuracy: 0.7100\n",
      "Epoch 136/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.2035 - accuracy: 0.9305 - val_loss: 0.5239 - val_accuracy: 0.7786\n",
      "Epoch 137/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.2082 - accuracy: 0.9326 - val_loss: 0.8016 - val_accuracy: 0.7357\n",
      "Epoch 138/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1423 - accuracy: 0.9574 - val_loss: 2.3624 - val_accuracy: 0.6586\n",
      "Epoch 139/150\n",
      "102/104 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8790INFO:tensorflow:Assets written to: model/yawn_model\\model.139-0.90\\assets\n",
      "104/104 [==============================] - 8s 76ms/step - loss: 0.3204 - accuracy: 0.8803 - val_loss: 0.4638 - val_accuracy: 0.8986\n",
      "Epoch 140/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2394 - accuracy: 0.9181 - val_loss: 0.9042 - val_accuracy: 0.6471\n",
      "Epoch 141/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1757 - accuracy: 0.9456 - val_loss: 2.0050 - val_accuracy: 0.6914\n",
      "Epoch 142/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.2076 - accuracy: 0.9287 - val_loss: 0.8098 - val_accuracy: 0.7114\n",
      "Epoch 143/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1982 - accuracy: 0.9323 - val_loss: 1.3875 - val_accuracy: 0.5829\n",
      "Epoch 144/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1956 - accuracy: 0.9374 - val_loss: 1.3819 - val_accuracy: 0.6743\n",
      "Epoch 145/150\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 0.1734 - accuracy: 0.9480 - val_loss: 1.0028 - val_accuracy: 0.7171\n",
      "Epoch 146/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.1483 - accuracy: 0.9541 - val_loss: 0.9762 - val_accuracy: 0.7286\n",
      "Epoch 147/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.1450 - accuracy: 0.9531 - val_loss: 2.1223 - val_accuracy: 0.6929\n",
      "Epoch 148/150\n",
      "104/104 [==============================] - 1s 8ms/step - loss: 0.1933 - accuracy: 0.9229 - val_loss: 0.5645 - val_accuracy: 0.7557\n",
      "Epoch 149/150\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.1607 - accuracy: 0.9444 - val_loss: 0.9269 - val_accuracy: 0.6843\n",
      "Epoch 150/150\n",
      "104/104 [==============================] - 1s 9ms/step - loss: 0.2378 - accuracy: 0.9102 - val_loss: 0.9438 - val_accuracy: 0.7014\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(X_train,Y_train, epochs=150, batch_size=32, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
